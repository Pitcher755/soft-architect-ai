# ğŸ”´ Redis Caching Patterns: High-Speed Data Access

> **Rol:** Cache de Datos en Memoria (Key-Value)
> **Motor:** Almacenamiento Datos VolÃ¡til (RAM)
> **Objetivo:** Reducir latencia de DB (100ms â†’ 1ms)
> **FilosofÃ­a:** "La cachÃ© es un espejo, no la fuente de verdad. Si se rompe, vuelves a la DB"
> **Estado:** âœ… Establecido
> **Fecha:** 30/01/2026

---

## ğŸ“– Tabla de Contenidos

1. [Fundamental: CachÃ© NO es Base de Datos](#fundamental-cachÃ©-no-es-base-de-datos)
2. [Cache-Aside Pattern (Recomendado)](#cache-aside-pattern-recomendado)
3. [Write-Through Pattern](#write-through-pattern)
4. [Write-Behind Pattern](#write-behind-pattern)
5. [TTL Strategies](#ttl-strategies)
6. [Problema: Hot Keys](#problema-hot-keys)
7. [Problema: Cache Stampede](#problema-cache-stampede)
8. [Keys Naming Convention](#keys-naming-convention)
9. [SerializaciÃ³n: JSON vs MessagePack](#serializaciÃ³n-json-vs-messagepack)
10. [Redis Pub/Sub vs Streams](#redis-pubsub-vs-streams)
11. [Monitoreo y Troubleshooting](#monitoreo-y-troubleshooting)

---

## Fundamental: CachÃ© NO es Base de Datos

### La Regla de Oro

**Redis es VOLÃTIL.** Puede perder todos los datos en cualquier momento:
- Reinicio del servidor
- OOM (Out of Memory) - evicciÃ³n de claves
- Failure del nodo

```
ğŸš¨ NUNCA hagas esto:

âŒ INCORRECTO
CREATE TABLE events (
    id SERIAL PRIMARY KEY
);

App -> Redis (guardar)
App -> Intenta leer de Redis
âŒ Redis estÃ¡ caÃ­do
âŒ Eventos perdidos

âœ… CORRECTO
CREATE TABLE events (
    id SERIAL PRIMARY KEY,
    created_at TIMESTAMP
);

App -> PostgreSQL (guardar primero)
App -> Redis (cache opcional)
App -> Intenta leer
   âœ… Redis hit: Devuelve desde cachÃ© (rÃ¡pido)
   âŒ Redis miss: Lee de DB, llena cachÃ© (lento pero funciona)
```

### Arquitectura TÃ­pica

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Application â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                    â”‚
   â†“                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Redis      â”‚   â”‚  PostgreSQL  â”‚
â”‚  (CachÃ©)     â”‚   â”‚  (Verdad)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Regla:
- âœ… Leer de Redis primero
- âœ… Si miss, leer de DB
- âœ… Guardar SIEMPRE en DB primero
- âœ… Actualizar cachÃ© (o invalidar)
```

---

## Cache-Aside Pattern (Recomendado)

### PatrÃ³n EstÃ¡ndar en SoftArchitect

La aplicaciÃ³n es responsable de leer/escribir en cachÃ©. Redis es pasivo.

### Flujo de Lectura (Get)

```python
def get_user(user_id: int) -> User:
    cache_key = f"users:{user_id}:profile"

    # Paso 1: Buscar en cachÃ©
    cached = redis.get(cache_key)
    if cached:
        return User.parse_obj(json.loads(cached))  # âœ… Hit (1ms)

    # Paso 2: Cache miss â†’ Leer de DB
    user = db.query(User).filter(User.id == user_id).first()
    if not user:
        return None  # 404

    # Paso 3: Guardar en cachÃ© con TTL
    redis.setex(
        cache_key,
        ttl=3600,  # 1 hora
        value=json.dumps(user.dict())
    )

    return user  # ~100ms (lectura DB)
```

**Resultado:**
- Hit rate 90%: Latencia promedio â‰ˆ 1ms Ã— 0.9 + 100ms Ã— 0.1 = **10ms** âœ…
- Hit rate 50%: Latencia promedio = 1ms Ã— 0.5 + 100ms Ã— 0.5 = **50ms** âš ï¸

### Flujo de Escritura (Set)

```python
def update_user(user_id: int, data: UserUpdate):
    # Paso 1: Actualizar en DB primero (verdad)
    user = db.query(User).filter(User.id == user_id).first()
    for field, value in data.dict(exclude_unset=True).items():
        setattr(user, field, value)
    db.commit()

    # Paso 2: INVALIDAR cachÃ© (borrar, no actualizar)
    cache_key = f"users:{user_id}:profile"
    redis.delete(cache_key)  # âœ… Borrar es mÃ¡s seguro que actualizar

    return user
```

**Â¿Por quÃ© BORRAR y no ACTUALIZAR?**

```
Escenario: Race condition con actualizaciÃ³n

âŒ Si actualizas el cachÃ©:
T1: API recibe request "cambiar email"
T2: DB actualiza email a "nuevo@example.com"
T3: CachÃ© se actualiza a "nuevo@example.com"
T4: Pero en T1.5 llegÃ³ otra request del mismo usuario
T5: Lee cachÃ© ("nuevo@example.com") pero DB tiene "viejo@example.com"
ğŸš¨ Inconsistencia!

âœ… Si borras el cachÃ©:
T1: API recibe request "cambiar email"
T2: DB actualiza email a "nuevo@example.com"
T3: CachÃ© se borra
T4: Siguiente lectura = miss â†’ lee DB ("nuevo@example.com")
T5: CachÃ© se llena con valor correcto
âœ… Siempre consistente!
```

---

## Write-Through Pattern

### MÃ¡s Seguro pero MÃ¡s Lento

Escribir SIMULTANEAMENTE en cachÃ© y DB. Esperar respuesta de ambos.

```python
def write_through_update(user_id: int, data: UserUpdate):
    cache_key = f"users:{user_id}:profile"

    # Paso 1: Actualizar DB
    user = db.query(User).filter(User.id == user_id).first()
    for field, value in data.dict(exclude_unset=True).items():
        setattr(user, field, value)
    db.commit()

    # Paso 2: Actualizar cachÃ© (mismo valor que DB)
    redis.setex(
        cache_key,
        ttl=3600,
        value=json.dumps(user.dict())
    )

    # Nota: Esperar a que ambos terminen
    return user
```

**Ventaja:**
- âœ… CachÃ© siempre estÃ¡ actualizado
- âœ… No hay penalidad en el siguiente read (siempre hit)

**Desventaja:**
- âŒ MÃ¡s lento (esperar cachÃ© + DB)
- âŒ Si Redis falla, escritura falla

### CuÃ¡ndo Usar

```
Usar Write-Through si:
- Escrituras son raras (ej: cambio de perfil usuario)
- Datos crÃ­ticos (no puedes perder o desincronizar)
- Baja latencia aceptable en escritura (~50-100ms)

Usar Cache-Aside si:
- Escrituras son frecuentes (ej: logs, mÃ©tricas)
- Datos no crÃ­ticos
- Baja latencia en escritura es crÃ­tico (~10ms)
```

---

## Write-Behind Pattern

### âš ï¸ Riesgoso: Write-Behind = Write to Cache, Async Write to DB

```python
def write_behind_dangerous(user_id: int, data: UserUpdate) -> User:
    cache_key = f"users:{user_id}:profile"

    # Paso 1: Escribir SOLO en cachÃ© (rÃ¡pido)
    user_dict = {"id": user_id, **data.dict()}
    redis.setex(cache_key, ttl=3600, value=json.dumps(user_dict))

    # Paso 2: ASINCRONAMENTE escribir en DB (en background)
    background_job_queue.enqueue(
        task=write_to_db,
        user_id=user_id,
        data=data
    )

    # Devolver inmediatamente (ultra-rÃ¡pido)
    return User(**user_dict)
```

**Ventaja:**
- âš¡ Ultra-rÃ¡pido (no esperar DB)
- âœ… Latencia < 5ms

**Desventaja:**
- ğŸš¨ **MUY RIESGOSO**: Si Redis falla antes de escribir en DB â†’ datos perdidos
- ğŸš¨ Inconsistencias temporales
- ğŸš¨ DifÃ­cil de debuggear

**Regla:** Usar SOLO para datos no crÃ­ticos (logs, analytics, telemetrÃ­a).

```python
# âœ… OK para uso no crÃ­tico
def log_user_action_write_behind(user_id: int, action: str):
    redis.lpush(f"events:queue", json.dumps({"user_id": user_id, "action": action}))
    # Background worker escribe en DB eventualmente
    # Si pierde algunos events, no es catastrÃ³fico

# âŒ NO para datos crÃ­ticos
def charge_payment_write_behind(user_id: int, amount: float):
    # âŒ NUNCA hacer esto: dinero perdido = demanda legal
    pass
```

---

## TTL Strategies

### Regla: NUNCA guardar sin TTL (a menos que sea inmutable)

```sql
-- âŒ INCORRECTO: Sin TTL
SET users:100:profile '{"name": "Alice", "age": 30}'

-- Redis nunca borra. Si tienes 1M usuarios â†’ RAM llena â†’ ğŸ’¥ OOM

-- âœ… CORRECTO: Con TTL
SETEX users:100:profile 3600 '{"name": "Alice", "age": 30}'

-- Redis borra automÃ¡ticamente despuÃ©s de 3600 segundos
```

### Estrategia por Tipo de Dato

| Tipo de Dato | TTL Recomendado | JustificaciÃ³n |
|:---|:---|:---|
| **User Profile** | 1-6 horas | Cambios raros, datos estables |
| **Session Token** | 30 dÃ­as | ExpiraciÃ³n de sesiÃ³n |
| **CotizaciÃ³n de Acciones** | 1-5 minutos | Datos volÃ¡tiles, actualizan frecuente |
| **Conteo de Vistas** | 24 horas | AgregaciÃ³n, baja frecuencia |
| **Producto en CatÃ¡logo** | 1 hora | Cambios ocasionales |
| **ConfiguraciÃ³n App** | 12 horas | Muy estable |
| **OTP (One-Time Pass)** | 10 minutos | CrÃ­tico, tiempo limitado |

### ImplementaciÃ³n en FastAPI + Redis

```python
from redis import Redis
from datetime import timedelta
import json

redis = Redis(host='localhost', port=6379, db=0)

async def cache_with_ttl(key: str, value: any, ttl_seconds: int = 3600):
    """Guardar en cachÃ© con TTL automÃ¡tico"""
    redis.setex(
        key,
        time=ttl_seconds,
        value=json.dumps(value, default=str)
    )

async def get_cached_or_db(key: str, ttl: int, db_fetch_fn):
    """PatrÃ³n Cache-Aside con TTL"""
    # Buscar en cachÃ©
    cached = redis.get(key)
    if cached:
        return json.loads(cached)

    # Miss: ir a DB
    data = await db_fetch_fn()

    # Guardar con TTL
    await cache_with_ttl(key, data, ttl)
    return data

# Uso
@app.get("/users/{user_id}")
async def get_user(user_id: int):
    return await get_cached_or_db(
        key=f"users:{user_id}",
        ttl=3600,  # 1 hora
        db_fetch_fn=lambda: db.query(User).filter(User.id == user_id).first()
    )
```

---

## Problema: Hot Keys

### Â¿QuÃ© es un Hot Key?

Una Ãºnica clave accedida por **cientos o miles de requests/segundo**.

```
Ejemplo: Black Friday, producto "MacBook Pro" 50% off

Key: products:123456:hot_item
Acceso normal: 10 requests/segundo
Durante venta: 10,000 requests/segundo

âŒ Problema:
- Single Redis node puede saturarse
- Network bandwidth agotada
- Redis devuelve lentamente
- Usuarios ven timeout
```

### SoluciÃ³n 1: Replicar CachÃ© Localmente

```python
from functools import lru_cache
from datetime import datetime, timedelta

class LocalCache:
    def __init__(self):
        self.cache = {}
        self.expire = {}

    def get(self, key: str):
        if key in self.cache:
            if self.expire[key] > datetime.now():
                return self.cache[key]
            else:
                del self.cache[key]  # Expirado
        return None

    def set(self, key: str, value, ttl_seconds: int):
        self.cache[key] = value
        self.expire[key] = datetime.now() + timedelta(seconds=ttl_seconds)

local_cache = LocalCache()

async def get_hot_product(product_id: int):
    # Paso 1: CachÃ© local (en memoria, muy rÃ¡pido)
    local_value = local_cache.get(f"products:{product_id}")
    if local_value:
        return local_value  # ~0.1ms (sin network)

    # Paso 2: Redis (si local miss)
    redis_value = redis.get(f"products:{product_id}")
    if redis_value:
        # Llenar cachÃ© local
        local_cache.set(f"products:{product_id}", redis_value, ttl=60)
        return redis_value  # ~10ms

    # Paso 3: DB (si Redis miss)
    db_value = db.query(Product).filter(Product.id == product_id).first()
    # Llenar Redis y local
    redis.setex(f"products:{product_id}", 3600, json.dumps(db_value.dict()))
    local_cache.set(f"products:{product_id}", db_value, ttl=3600)
    return db_value  # ~100ms
```

### SoluciÃ³n 2: Sharding

Distribuir hot keys entre mÃºltiples Redis nodes.

```python
def get_shard_id(key: str, num_shards: int) -> int:
    """Determinar quÃ© shard (Redis node) usar"""
    return hash(key) % num_shards

# Redis nodes
redis_shards = [
    Redis(host='redis-1', port=6379),
    Redis(host='redis-2', port=6379),
    Redis(host='redis-3', port=6379),
]

def get_from_cache(key: str):
    shard_id = get_shard_id(key, len(redis_shards))
    redis_node = redis_shards[shard_id]
    return redis_node.get(key)

# DistribuciÃ³n:
# products:123456 â†’ hash â†’ shard 0
# products:123457 â†’ hash â†’ shard 1
# products:123458 â†’ hash â†’ shard 2
# Carga distribuida âœ…
```

---

## Problema: Cache Stampede

### Â¿QuÃ© es?

Multiple procesos intentan refrescar la misma cachÃ© expirada simultÃ¡neamente â†’ **todos van a DB** â†’ sobrecarga.

```
Timeline:

10:00:00 - CachÃ© se expira
10:00:01 - 1000 requests llegan simultÃ¡neamente
10:00:01 - TODOS ven cache miss
10:00:01 - TODOS van a DB
10:00:01 - DB recibe 1000 queries de golpe ğŸ’¥
10:00:05 - DB se satura, timeout
```

### SoluciÃ³n: Distributed Lock

```python
import asyncio
from redis import Redis

redis = Redis(host='localhost', port=6379)

async def get_with_lock(key: str, ttl: int, db_fetch_fn):
    """Cache-Aside con lock distribuido para evitar cache stampede"""

    # Paso 1: Buscar en cachÃ©
    cached = redis.get(key)
    if cached:
        return json.loads(cached)

    # Paso 2: Lock (evitar que otros procesos vayan a DB)
    lock_key = f"{key}:lock"
    lock = redis.lock(lock_key, timeout=10)  # Lock por 10 segundos max

    if lock.acquire(blocking=False):  # Non-blocking
        try:
            # Paso 3: Yo obtuve el lock, ir a DB
            data = await db_fetch_fn()

            # Paso 4: Guardar en cachÃ©
            redis.setex(key, ttl, json.dumps(data, default=str))

            return data
        finally:
            lock.release()
    else:
        # Paso 5: Otro proceso tiene el lock, esperar y reintentar
        for i in range(50):  # Esperar max 5 segundos (50 Ã— 100ms)
            await asyncio.sleep(0.1)
            cached = redis.get(key)
            if cached:
                return json.loads(cached)

        # Si todavÃ­a no hay cachÃ©, ir a DB de todas formas
        return await db_fetch_fn()

# Resultado: Solo 1 de 1000 requests va a DB ğŸ¯
```

**Resultado:**
- Sin lock: 1000 queries a DB ğŸ’¥
- Con lock: 1 query a DB âœ… (99.9% reducciÃ³n)

---

## Keys Naming Convention

### EstÃ¡ndar: Namespace con Dos Puntos

```
Formato: <namespace>:<entity_type>:<entity_id>:<attribute>

Ejemplos:

users:100:profile              # Perfil de usuario 100
users:100:settings            # Preferencias de usuario 100
auth:tokens:abc123xyz         # Token de autenticaciÃ³n
sessions:sid:xyz              # SesiÃ³n
cache:products:list:page:1    # Producto lista paginada

cache:reports:sales:monthly:2026-01  # Reporte mensual

inventory:warehouse:001:stock  # Stock en almacÃ©n 1
```

### Ventajas

```python
# 1. FÃ¡cil de entender
key = "users:100:profile"  # Claro: usuario 100, su perfil

# 2. FÃ¡cil de agrupar (con SCAN)
redis.scan_iter("users:*")  # Todas las claves de usuarios
redis.scan_iter("users:100:*")  # Todas las claves del usuario 100

# 3. Invalidar grupos
redis.delete("users:100:*")  # Borrar todo del usuario 100

# 4. Monitoreo y debugging
redis_cli> KEYS "cache:*"  # Ver todas las claves de cachÃ©
```

---

## SerializaciÃ³n: JSON vs MessagePack

### JSON (Defecto, Legible)

```python
import json

# Ventajas
# âœ… Legible en CLI
# âœ… Compatible con cualquier lenguaje
# âœ… FÃ¡cil debuggear

redis.set("users:100", json.dumps({"name": "Alice", "age": 30}))
redis_cli> GET users:100
# Resultado: {"name":"Alice","age":30}  âœ… Legible

# Desventajas
# âŒ MÃ¡s lento (parsing texto)
# âŒ MÃ¡s tamaÃ±o (mÃ¡s bytes)
```

### MessagePack (RÃ¡pido, Compacto)

```python
import msgpack

# Ventajas
# âœ… 2-3x mÃ¡s rÃ¡pido que JSON
# âœ… 30-40% mÃ¡s pequeÃ±o
# âœ… Ideal para cachÃ© con mucho volumen

data = {"name": "Alice", "age": 30}
packed = msgpack.packb(data)

redis.set("users:100", packed)
# Resultado: bytes (no legible en CLI)

unpacked = msgpack.unpackb(redis.get("users:100"))
# Resultado: {"name": "Alice", "age": 30} âœ…

# Desventajas
# âŒ No legible directamente
# âŒ Necesita librerÃ­a
```

### RecomendaciÃ³n

```
Usa JSON si:
- Datos pequeÃ±os (<1KB)
- Debugging es importante
- Compatible con mÃºltiples lenguajes

Usa MessagePack si:
- Alto volumen (>100K claves)
- Datos frecuentemente accedidos
- PreocupaciÃ³n por RAM
```

---

## Redis Pub/Sub vs Streams

### Pub/Sub: Fire-and-Forget (EfÃ­mero)

```python
# Publicador
redis.publish("notifications", json.dumps({
    "user_id": 100,
    "message": "Tu pedido fue enviado"
}))

# Suscriptor
pubsub = redis.pubsub()
pubsub.subscribe("notifications")

for message in pubsub.listen():
    if message['type'] == 'message':
        data = json.loads(message['data'])
        print(f"NotificaciÃ³n: {data['message']}")
```

**CaracterÃ­sticas:**
- âœ… RÃ¡pido (fire-and-forget)
- âŒ No persistente (si suscriptor no estÃ¡, pierde mensaje)
- âœ… Ideal para: Websocket broadcasts, real-time updates

### Streams: Cola Persistente (Recomendado)

```python
# Productor
redis.xadd("email-queue", {
    "to": "alice@example.com",
    "subject": "Bienvenido",
    "body": "Gracias por registrarte"
})

# Consumidor
messages = redis.xread({"email-queue": "0"}, count=10)

for stream_key, messages_list in messages:
    for message_id, data in messages_list:
        to_email = data[b'to'].decode()
        send_email(to_email, data)

        # Marcar como procesado
        redis.xack("email-queue", "my_group", message_id)
```

**CaracterÃ­sticas:**
- âœ… Persistente (si consumer falla, reinicia desde Ãºltimo mensaje)
- âœ… Grupos de consumidores (distribuciÃ³n de carga)
- âœ… Ideal para: Colas de trabajos, event sourcing

**CuÃ¡ndo usar quÃ©:**

| Caso de Uso | Pub/Sub | Streams |
|:---|:---:|:---:|
| Real-time Websocket broadcast | âœ… | âŒ |
| Email queue | âŒ | âœ… |
| Notificaciones push | âŒ | âœ… |
| MÃ©tricas en vivo | âœ… | âŒ |
| Worker pool de trabajos | âŒ | âœ… |

---

## Monitoreo y Troubleshooting

### 1. Health Check

```python
from redis import Redis
from redis.exceptions import ConnectionError

def redis_health():
    try:
        redis.ping()
        info = redis.info()
        return {
            "status": "healthy",
            "used_memory_mb": info['used_memory'] / 1024 / 1024,
            "connected_clients": info['connected_clients']
        }
    except ConnectionError:
        return {"status": "down"}

# En FastAPI
@app.get("/health/redis")
async def health():
    return redis_health()
```

### 2. Monitorear RAM

```bash
# Conectar a Redis CLI
redis-cli

# Ver memoria
> INFO memory
used_memory:1073741824  # 1GB
maxmemory:2147483648   # 2GB (lÃ­mite)
maxmemory_policy:allkeys-lru  # PolÃ­tica de evicciÃ³n

# Ver claves mÃ¡s grandes
> SCAN 0 COUNT 1000
# Revisar manualmente con SIZE

# Ver acceso por clave
> SLOWLOG GET 10  # Top 10 queries lentas
```

### 3. Hit Rate

```python
def redis_stats():
    info = redis.info()

    hits = info.get('keyspace_hits', 0)
    misses = info.get('keyspace_misses', 0)

    if hits + misses == 0:
        hit_rate = 0
    else:
        hit_rate = 100 * hits / (hits + misses)

    return {
        "hit_rate_percent": hit_rate,
        "hits": hits,
        "misses": misses
    }

# Target: > 80% hit rate
# Si < 60%, revisar TTL, tamaÃ±o de cachÃ©
```

### 4. Evitar OOM (Out of Memory)

```
Configurar en redis.conf:

# MÃ¡xima memoria permitida
maxmemory 2gb

# PolÃ­tica de evicciÃ³n cuando llena:
# allkeys-lru: Borrar cualquier clave LRU (recomendado)
# volatile-lru: Borrar clave con TTL LRU
# noeviction: Error (rechazar nuevas escrituras)
maxmemory-policy allkeys-lru
```

---

## Resumen: Patrones de CachÃ© en SoftArchitect

| PatrÃ³n | Lectura | Escritura | Consistencia | Riesgo | RecomendaciÃ³n |
|:---|:---|:---|:---|:---|:---|
| **Cache-Aside** | âš¡âš¡âš¡ | âš¡âš¡ | Media | Bajo | âœ… **ESTÃNDAR** |
| **Write-Through** | âš¡âš¡âš¡ | âš¡ | Alta | Bajo | âš ï¸ Datos crÃ­ticos |
| **Write-Behind** | âš¡âš¡âš¡ | âš¡âš¡âš¡ | Baja | Alto | âŒ Datos no crÃ­ticos |

**Goal:** Cache-Aside con:
- âœ… TTL por tipo de dato
- âœ… Local cache para hot keys
- âœ… Distributed lock para cache stampede
- âœ… > 80% hit rate
- âœ… < 1GB RAM para cachÃ© tÃ­pico

Latencia objetivo: **P95 < 50ms** (redis hit + DB miss) ğŸš€

---

## Ejemplo Completo: FastAPI + Redis

```python
from fastapi import FastAPI, HTTPException
from redis import Redis
from sqlalchemy import create_engine, Column, Integer, String
from sqlalchemy.orm import sessionmaker, declarative_base
import json

app = FastAPI()
redis = Redis(host='localhost', port=6379, db=0)

Base = declarative_base()
engine = create_engine("postgresql://localhost/mydb")
SessionLocal = sessionmaker(bind=engine)

class User(Base):
    __tablename__ = "users"
    id = Column(Integer, primary_key=True)
    name = Column(String)
    email = Column(String)

# Cache-Aside Pattern
@app.get("/users/{user_id}")
async def get_user(user_id: int):
    cache_key = f"users:{user_id}:profile"

    # Step 1: Try cache
    cached = redis.get(cache_key)
    if cached:
        return json.loads(cached)

    # Step 2: Cache miss â†’ DB
    db = SessionLocal()
    user = db.query(User).filter(User.id == user_id).first()
    if not user:
        raise HTTPException(status_code=404)

    # Step 3: Fill cache
    redis.setex(
        cache_key,
        3600,  # 1 hour TTL
        json.dumps({"id": user.id, "name": user.name, "email": user.email})
    )

    return {"id": user.id, "name": user.name, "email": user.email}

# Invalidate cache on update
@app.put("/users/{user_id}")
async def update_user(user_id: int, name: str, email: str):
    db = SessionLocal()
    user = db.query(User).filter(User.id == user_id).first()
    if not user:
        raise HTTPException(status_code=404)

    user.name = name
    user.email = email
    db.commit()

    # Invalidate cache
    redis.delete(f"users:{user_id}:profile")

    return {"id": user.id, "name": user.name, "email": user.email}
```

ğŸ”´ **Redis CachÃ© Patterns Completado.** ğŸ’¾âœ¨
