# üÜî Tech Profile: LangChain (Python)

> **Categor√≠a:** LLM Orchestration Framework
> **Versi√≥n Objetivo:** 0.1.0+ (LCEL Stable)
> **Licencia:** MIT

El marco est√°ndar para construir aplicaciones conscientes del contexto (Context-Aware) y razonamiento (Reasoning).

---

## üìñ Tabla de Contenidos

1. [Casos de Uso (Suitability)](#casos-de-uso-suitability)
2. [An√°lisis de Valor (LCEL)](#an√°lisis-de-valor-lcel)
3. [Requisitos del Sistema](#requisitos-del-sistema)
4. [Decisi√≥n de Adopci√≥n](#decisi√≥n-de-adopci√≥n)

---

## Casos de Uso (Suitability)

### ‚úÖ Ideal Para (Best Fit)

* **RAG Pipelines Complejos:** Cadenas que requieren recuperaci√≥n, re-ranking, historial de chat y citaci√≥n de fuentes.
* **Salidas Estructuradas:** Convertir texto de LLM en JSON validado (Pydantic) para uso en APIs.
* **Agentes:** Sistemas que deciden qu√© herramientas usar (B√∫squeda, Calculadora, API) bas√°ndose en input del usuario.
* **Razonamiento Multi-Paso:** Cadenas que requieren reflexi√≥n iterativa, verificaci√≥n de salidas o planificaci√≥n.

### ‚ùå No Usar Para (Anti-Patterns)

* **Llamadas Simples (One-shot):** Si solo necesitas enviar un prompt y recibir un string, usa el SDK del proveedor (OpenAI/Anthropic) o `requests` a Ollama. LangChain a√±ade overhead innecesario aqu√≠.
* **Producci√≥n Cr√≠tica de Latencia (<20ms):** La abstracci√≥n de LangChain a√±ade milisegundos. Para High-Frequency Trading con LLMs, usa drivers nativos.
* **Modelos Peque√±os Embebidos:** Si tu modelo es un `distilbert` para clasificaci√≥n, LangChain es overkill.

---

## An√°lisis de Valor (LCEL)

La verdadera potencia reside en **LCEL (LangChain Expression Language)**:

* **Streaming Nativo:** Cualquier cadena construida con LCEL soporta `.stream()` y `.astream()` autom√°ticamente. Sin c√≥digo extra.
* **Paralelismo:** Ejecuta pasos independientes en paralelo sin c√≥digo extra (`RunnableParallel`).
* **Trazabilidad:** Integraci√≥n nativa con LangSmith para depurar pasos intermedios, examinar prompts exactos, ver latencias.
* **Composabilidad:** Cadenas son Runnables: `ChainA | ChainB | ChainC` crea autom√°ticamente una nueva cadena v√°lida.
* **TypeHints:** Las cadenas generadas con LCEL tienen type hints correctos (type-safe).

### Evoluci√≥n de LangChain

| Era | Patr√≥n | Estado |
|:---|:---|:---|
| **2023 (Legacy)** | `LLMChain(llm=..., prompt=...)` | ‚ùå Deprecado |
| **2024 (Presente)** | `prompt \| model \| parser` (LCEL) | ‚úÖ Est√°ndar Actual |
| **Futuro (Post-2025)** | Posiblemente Semantic Kernel v1.0 | ‚è≥ Monitorear |

**Decisi√≥n SoftArchitect:** Adoptar **LCEL exclusivamente**. Rechazar cualquier c√≥digo que use `LLMChain` en PRs.

---

## Requisitos del Sistema

### Paquetes Python

```bash
# Core
pip install langchain>=0.1.0
pip install langchain-core>=0.1.0

# Comunidad (Integraciones)
pip install langchain-community>=0.0.10

# Proveedores LLM (elige seg√∫n stack)
pip install langchain-openai    # OpenAI GPT-4/3.5
pip install langchain-anthropic # Claude
pip install ollama              # Para Ollama local

# Vector Stores
pip install chromadb            # Local
pip install pinecone-client     # Cloud
```

### Versi√≥n Python

* **M√≠nima:** 3.10 (type hints modernos)
* **Recomendada:** 3.11+ (performance en `asyncio`)

### Dependencias Transversales

* `pydantic>=2.0` - Para esquemas estructurados
* `httpx` - Cliente async para APIs
* `tenacity` - Reintentos con exponential backoff

---

## Decisi√≥n de Adopci√≥n

‚úÖ **SoftArchitect adopta LangChain como orquestador LLM est√°ndar** bajo estas condiciones:

1. **Todas las cadenas usan LCEL** (pipe syntax `|`)
2. **Todos los outputs se validan con Pydantic**
3. **Streaming es la opci√≥n por defecto** para UX fluida
4. **LangSmith se integra en staging/prod** para observabilidad

---

**Fecha:** 30 de Enero de 2026
**Status:** ‚úÖ ADOPTADO
**Responsable:** ArchitectZero AI Agent
