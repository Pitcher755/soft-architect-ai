# ðŸ“ Project Structure: LangChain Integration

> **PropÃ³sito:** PatrÃ³n de organizaciÃ³n de cÃ³digo para aplicaciones LangChain
> **FilosofÃ­a:** Feature Slices + Clean Architecture

---

## Estructura Recomendada

```
soft-architect-ai/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py            # âš™ï¸ ConfiguraciÃ³n de Modelos (Temp, Top-K, API Keys)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_provider.py      # ðŸ­ Factory para obtener ChatOpenAI / ChatOllama / ChatAnthropic
â”‚   â”‚   â”‚   â””â”€â”€ models.py            # ðŸ“Š Enum de modelos disponibles
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ prompts/                 # ðŸ“ Templates de prompts (puros, reutilizables)
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ rag_prompts.py       # RAG-specific prompts
â”‚   â”‚   â”‚   â”œâ”€â”€ agent_prompts.py     # Agent-specific prompts
â”‚   â”‚   â”‚   â”œâ”€â”€ summarization_prompts.py
â”‚   â”‚   â”‚   â””â”€â”€ translation_prompts.py
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ schemas/                 # ðŸ“‹ Esquemas Pydantic para outputs
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ common.py            # Schemas genÃ©ricos (Citation, Error)
â”‚   â”‚       â”œâ”€â”€ rag_schemas.py       # Para RAG outputs
â”‚   â”‚       â””â”€â”€ agent_schemas.py     # Para Agent outputs
â”‚   â”‚
â”‚   â”œâ”€â”€ features/                    # ðŸŽ¯ Feature Slices (cada feature aislada)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ document_chat/           # Feature: Chat sobre documentos
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ chains/              # ðŸ”— LÃ³gica de OrquestaciÃ³n (LCEL)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ retrieval_chain.py    # Buscar docs + pasar al LLM
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ generation_chain.py   # Generar respuesta basada en docs
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ combined_chain.py     # Retrieval + Generation (RAG completo)
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ tools/               # ðŸ› ï¸ Herramientas para Agentes
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ vector_search_tool.py # Tool: buscar en vector store
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ citation_tool.py      # Tool: extraer fuentes
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ parsers/             # ðŸ“¦ Pydantic Parsers
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ response_parser.py   # Parser: validar response estructura
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ service.py           # ðŸŽ­ Service layer (inyecta chains + herramientas)
â”‚   â”‚   â”‚   â”œâ”€â”€ models.py            # ðŸ“Š Domain models
â”‚   â”‚   â”‚   â””â”€â”€ repository.py        # ðŸ’¾ Data access
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ invoice_extraction/      # Feature: ExtracciÃ³n de facturas
â”‚   â”‚   â”‚   â”œâ”€â”€ chains/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ extraction_chain.py
â”‚   â”‚   â”‚   â”œâ”€â”€ parsers/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ invoice_parser.py
â”‚   â”‚   â”‚   â””â”€â”€ service.py
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ code_generation/         # Feature: Generar cÃ³digo
â”‚   â”‚       â”œâ”€â”€ chains/
â”‚   â”‚       â”œâ”€â”€ tools/
â”‚   â”‚       â””â”€â”€ service.py
â”‚   â”‚
â”‚   â””â”€â”€ infrastructure/
â”‚       â”œâ”€â”€ vector_store/            # ðŸ—„ï¸ IntegraciÃ³n con Chroma/Pinecone
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ base_store.py        # Interfaz abstracta
â”‚       â”‚   â”œâ”€â”€ chroma_store.py      # ImplementaciÃ³n: Chroma local
â”‚       â”‚   â”œâ”€â”€ pinecone_store.py    # ImplementaciÃ³n: Pinecone cloud
â”‚       â”‚   â””â”€â”€ retriever.py         # Wrapper para usar como Runnable
â”‚       â”‚
â”‚       â”œâ”€â”€ cache/                   # ðŸ’¾ CachÃ© de responses (opcional)
â”‚       â”‚   â””â”€â”€ redis_cache.py
â”‚       â”‚
â”‚       â””â”€â”€ observability/           # ðŸ“Š Logging + LangSmith
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ logger_config.py     # Setup de logging
â”‚           â””â”€â”€ langsmith_config.py  # Setup de LangSmith tracer
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â””â”€â”€ document_chat/
â”‚   â”‚       â”œâ”€â”€ test_retrieval_chain.py
â”‚   â”‚       â”œâ”€â”€ test_generation_chain.py
â”‚   â”‚       â””â”€â”€ test_service.py
â”‚   â”‚
â”‚   â””â”€â”€ integration/
â”‚       â””â”€â”€ test_rag_e2e.py
â”‚
â””â”€â”€ pyproject.toml                    # ðŸ“¦ Dependencies (langchain, pydantic, etc)

```

---

## Desglose por Responsabilidad

### `core/llm/` - ConfiguraciÃ³n LLM Centralizada

**PropÃ³sito:** Lugar Ãºnico donde se instancian y configuran modelos.

```python
# core/llm/config.py
from dataclasses import dataclass

@dataclass
class LLMConfig:
    temperature: float = 0.7
    max_tokens: int = 2000
    timeout: int = 30

# core/llm/llm_provider.py
from langchain_community.chat_models import ChatOllama
from langchain_openai import ChatOpenAI

class LLMProvider:
    @staticmethod
    def get_model(provider: str = "ollama", config: LLMConfig = LLMConfig()):
        if provider == "ollama":
            return ChatOllama(
                model="llama3",
                temperature=config.temperature,
                num_predict=config.max_tokens
            )
        elif provider == "openai":
            return ChatOpenAI(
                model="gpt-4",
                temperature=config.temperature,
                max_tokens=config.max_tokens
            )
        else:
            raise ValueError(f"Unknown provider: {provider}")

# Usage en cualquier chain:
model = LLMProvider.get_model("ollama")
```

### `core/prompts/` - Templates Separados

**PropÃ³sito:** Prompts documentados, versionables, testeables.

```python
# core/prompts/rag_prompts.py
from langchain_core.prompts import ChatPromptTemplate

RAG_SYSTEM_PROMPT = """You are a helpful assistant answering questions based on provided context.
Instructions:
- Use ONLY the context provided
- If answer not in context, say "No information available"
- Provide accurate citations"""

RAG_CONTEXT_TEMPLATE = """Context documents:\n{context}"""
RAG_QUESTION_TEMPLATE = """Question: {question}"""

rag_prompt = ChatPromptTemplate.from_messages([
    ("system", RAG_SYSTEM_PROMPT),
    ("user", RAG_CONTEXT_TEMPLATE + "\n\n" + RAG_QUESTION_TEMPLATE)
])
```

### `core/schemas/` - Pydantic Models

**PropÃ³sito:** Documentar y validar estructuras de output.

```python
# core/schemas/rag_schemas.py
from pydantic import BaseModel, Field
from typing import List

class Citation(BaseModel):
    source: str = Field(description="Documento fuente")
    page: int = Field(description="NÃºmero de pÃ¡gina")
    excerpt: str = Field(description="Fragmento citado")

class RAGResponse(BaseModel):
    answer: str = Field(description="Respuesta completa")
    citations: List[Citation] = Field(description="Fuentes citadas")
    confidence: float = Field(ge=0, le=1, description="Confianza (0-1)")
```

### `features/{{FEATURE}}/` - Feature Slices

**PropÃ³sito:** Cada feature (RAG, Agents, etc) es autÃ³noma.

```
document_chat/
â”œâ”€â”€ chains/              # LÃ³gica pura LCEL
â”œâ”€â”€ tools/              # Si es agente, herramientas aquÃ­
â”œâ”€â”€ parsers/            # Parsers especÃ­ficos del feature
â”œâ”€â”€ service.py          # Orquesta chains + inyecta dependencias
â””â”€â”€ models.py           # Domain models del feature
```

**Ejemplo: `features/document_chat/service.py`**

```python
from core.llm.llm_provider import LLMProvider
from core.prompts.rag_prompts import rag_prompt
from core.schemas.rag_schemas import RAGResponse
from infrastructure.vector_store.retriever import get_retriever
from chains.combined_chain import build_rag_chain

class DocumentChatService:
    def __init__(self):
        self.model = LLMProvider.get_model("ollama")
        self.retriever = get_retriever()
        self.rag_chain = build_rag_chain(
            prompt=rag_prompt,
            model=self.model,
            retriever=self.retriever
        )

    async def chat(self, question: str) -> RAGResponse:
        result = await self.rag_chain.ainvoke({
            "question": question
        })
        return RAGResponse.model_validate(result)
```

### `infrastructure/vector_store/` - Persistencia

**PropÃ³sito:** AbstracciÃ³n de vector store (Chroma, Pinecone, etc).

```python
# infrastructure/vector_store/base_store.py
from abc import ABC, abstractmethod

class VectorStoreBase(ABC):
    @abstractmethod
    def add_documents(self, texts: List[str], metadata: List[dict]):
        pass

    @abstractmethod
    def as_retriever(self, search_type: str = "similarity"):
        pass

# infrastructure/vector_store/chroma_store.py
from langchain_community.vectorstores import Chroma

class ChromaStore(VectorStoreBase):
    def __init__(self, collection_name: str):
        self.store = Chroma(collection_name=collection_name)

    def as_retriever(self, search_type: str = "similarity"):
        return self.store.as_retriever(search_type=search_type)

# Usage:
retriever = ChromaStore("documents").as_retriever()
```

---

## Patrones de Uso

### Pattern 1: Simple RAG

```
core/llm (model config)
    â†“
core/prompts (prompt template)
    â†“
features/document_chat/chains (LCEL: retriever | prompt | model | parser)
    â†“
features/document_chat/service (orquestaciÃ³n)
```

### Pattern 2: Agent with Tools

```
core/llm (model config)
    â†“
core/prompts (system prompt para agente)
    â†“
features/agent/tools (herramientas disponibles)
    â†“
features/agent/chains (agent = model con tools)
    â†“
features/agent/service (loop: think â†’ act â†’ observe)
```

### Pattern 3: Multi-Feature

```
feature1/document_chat
    â””â”€â”€ chains/rag_chain.py
feature2/invoice_extraction
    â””â”€â”€ chains/extraction_chain.py
feature3/code_generation
    â””â”€â”€ chains/generation_chain.py

Todas comparten:
- core/llm (mismo modelo)
- core/prompts (templates reutilizables)
- infrastructure/vector_store (mismo retriever)
```

---

## Principios de DiseÃ±o

1. **SeparaciÃ³n de Concerns:** Prompts â‰  Logic â‰  Infrastructure
2. **ReutilizaciÃ³n:** Core modules se importan en features
3. **Testabilidad:** Cada chain es una funciÃ³n pura testeable
4. **Escalabilidad:** Agregar features sin modificar core
5. **Observabilidad:** Logs en infra/, no en chains

---

**Fecha:** 30 de Enero de 2026
**VersiÃ³n:** 1.0
**Status:** âœ… RECOMMENDED STRUCTURE
