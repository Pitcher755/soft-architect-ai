# ü¶ú LCEL Patterns: LangChain Expression Language

> **Paradigma:** Declarativo, Componible, Streamable
> **Sintaxis:** `Input | Prompt | Model | OutputParser`
> **Versi√≥n:** LangChain 0.1.0+ (LCEL Stable)

La unidad fundamental de trabajo en LangChain moderno.

---

## üìñ Tabla de Contenidos

1. [Patr√≥n B√°sico (Basic Chain)](#patr√≥n-b√°sico-basic-chain)
2. [Patr√≥n RAG (Retrieval Augmented Generation)](#patr√≥n-rag-retrieval-augmented-generation)
3. [Patr√≥n de Salida Estructurada (Pydantic)](#patr√≥n-de-salida-estructurada-pydantic)
4. [Patr√≥n Conversacional (Memory)](#patr√≥n-conversacional-memory)
5. [Patr√≥n Agente (ReAct Loop)](#patr√≥n-agente-react-loop)
6. [Patr√≥n Paralelo (Multithreading)](#patr√≥n-paralelo-multithreading)
7. [Patr√≥n Condicional (Branching)](#patr√≥n-condicional-branching)
8. [Anti-Patterns y Errores Comunes](#anti-patterns-y-errores-comunes)

---

## Patr√≥n B√°sico (Basic Chain)

La unidad fundamental de trabajo.

### Estructura

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.chat_models import ChatOllama

# 1Ô∏è‚É£ Componentes
model = ChatOllama(model="llama3", temperature=0.7)
prompt = ChatPromptTemplate.from_template(
    "Cu√©ntame un chiste sobre {topic}"
)
parser = StrOutputParser()

# 2Ô∏è‚É£ Composici√≥n (The Pipe)
chain = prompt | model | parser

# 3Ô∏è‚É£ Ejecuci√≥n Sync
result = chain.invoke({"topic": "programadores"})
print(result)  # String: "¬øPor qu√© los programadores..."

# 4Ô∏è‚É£ Ejecuci√≥n Async Stream (Token a token)
import asyncio

async def stream_chain():
    async for chunk in chain.astream({"topic": "Python"}):
        print(chunk, end="", flush=True)

asyncio.run(stream_chain())
```

### Desglose

| Componente | Rol | Ejemplo |
|:---|:---|:---|
| **Prompt** | Formatea input | `ChatPromptTemplate.from_template(...)` |
| **Model** | Genera token | `ChatOllama(model="llama3")` |
| **Parser** | Procesa output | `StrOutputParser()`, `PydanticOutputParser(...)` |

### Invocaciones

```python
# ‚úÖ Sincr√≥nico
result = chain.invoke({"topic": "AI"})

# ‚úÖ Asincr√≥nico
result = await chain.ainvoke({"topic": "AI"})

# ‚úÖ Stream (token a token)
for chunk in chain.stream({"topic": "AI"}):
    print(chunk, end="")

# ‚úÖ Async Stream
async for chunk in chain.astream({"topic": "AI"}):
    print(chunk, end="")

# ‚úÖ Batch (m√∫ltiples inputs)
results = chain.batch([
    {"topic": "AI"},
    {"topic": "Python"},
    {"topic": "LLMs"}
])
```

---

## Patr√≥n RAG (Retrieval Augmented Generation)

Usar `RunnableParallel` para buscar documentos y pasar la pregunta simult√°neamente.

### Estructura

```python
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.chat_models import ChatOllama
from langchain_community.vectorstores import Chroma

# Setup
model = ChatOllama(model="llama3")
vectorstore = Chroma(collection_name="documents")
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3})

# Prompt
rag_prompt = ChatPromptTemplate.from_template("""
You are a helpful assistant. Answer the question based on the context.

Context:
{context}

Question: {question}

Answer:""")

# RAG Chain con RunnableParallel
rag_chain = (
    RunnableParallel({
        "context": retriever,                  # Busca docs usando el input
        "question": RunnablePassthrough()      # Pasa el input tal cual
    })
    | rag_prompt                               # Recibe {context, question}
    | model
    | StrOutputParser()
)

# Uso
question = "¬øCu√°l es la capital de Francia?"
result = rag_chain.invoke({"question": question})
print(result)
```

### Desglose de RunnableParallel

```python
# RunnableParallel ejecuta m√∫ltiples runnables en PARALELO
parallel = RunnableParallel({
    "context": retriever,              # Rama 1: buscar (independiente del input)
    "question": RunnablePassthrough()  # Rama 2: pasar input sin cambios
})

# Input: "¬øCapital de Francia?"
# Output: {
#     "context": "Francia es un pa√≠s... Par√≠s es la capital...",
#     "question": "¬øCapital de Francia?"
# }

# Este output se pasa al siguiente paso (prompt)
```

### Alternativa: Buscar por Pregunta

```python
# Si el retriever necesita acceder solo a "question":
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    RunnableParallel({
        "context": retriever | format_docs,  # Extrae docs.page_content
        "question": RunnablePassthrough()
    })
    | rag_prompt
    | model
    | StrOutputParser()
)
```

---

## Patr√≥n de Salida Estructurada (Pydantic)

Obligar al LLM a devolver objetos Python v√°lidos.

### Estructura

```python
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate

# 1Ô∏è‚É£ Definir esquema
class Joke(BaseModel):
    setup: str = Field(description="La introducci√≥n del chiste")
    punchline: str = Field(description="El remate gracioso")

# 2Ô∏è‚É£ Crear parser
parser = PydanticOutputParser(pydantic_object=Joke)

# 3Ô∏è‚É£ Obtener instrucciones de formato
format_instructions = parser.get_format_instructions()

# 4Ô∏è‚É£ Crear prompt CON instrucciones
prompt = ChatPromptTemplate.from_template("""
Generate a joke about {topic}.

{format_instructions}
""")

# 5Ô∏è‚É£ Armar cadena
model = ChatOllama(model="llama3")
chain = prompt | model | parser

# 6Ô∏è‚É£ Invocar
joke_obj: Joke = chain.invoke({
    "topic": "AI",
    "format_instructions": format_instructions
})

print(joke_obj.punchline)  # ‚úÖ Type-safe: String
print(type(joke_obj))      # <class 'Joke'>
```

### Con Function Calling (GPT-4/Claude)

```python
# ‚úÖ MEJOR: Si el modelo soporta function calling
from langchain_openai import ChatOpenAI

class InvoiceData(BaseModel):
    invoice_number: str = Field(description="N√∫mero √∫nico")
    total_amount: float = Field(description="Monto total")
    due_date: str = Field(description="Fecha vencimiento (YYYY-MM-DD)")

model = ChatOpenAI(model="gpt-4")
structured_llm = model.with_structured_output(InvoiceData)

prompt = ChatPromptTemplate.from_template("""
Extract invoice data from:
{invoice_text}
""")

chain = prompt | structured_llm

invoice: InvoiceData = chain.invoke({"invoice_text": raw_text})
```

### Manejo de Errores

```python
from langchain_core.output_parsers import OutputParserException

try:
    joke_obj: Joke = chain.invoke({
        "topic": "AI",
        "format_instructions": format_instructions
    })
except OutputParserException as e:
    logger.error(f"Parse error: {e}")
    # Fallback
    joke_obj = Joke(
        setup="No puedo generar el chiste",
        punchline="Lo siento, intenta de nuevo"
    )

print(joke_obj)
```

---

## Patr√≥n Conversacional (Memory)

A√±adir historial de chat a una cadena LCEL.

### Estructura B√°sica

```python
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory

# 1Ô∏è‚É£ Crear la cadena base
base_chain = prompt | model | StrOutputParser()

# 2Ô∏è‚É£ Funci√≥n que obtiene historial por session_id
def get_session_history(session_id: str):
    # En prod: obtener de DB
    if not hasattr(get_session_history, "store"):
        get_session_history.store = {}

    if session_id not in get_session_history.store:
        get_session_history.store[session_id] = ChatMessageHistory()

    return get_session_history.store[session_id]

# 3Ô∏è‚É£ Envolver con RunnableWithMessageHistory
chain_with_history = RunnableWithMessageHistory(
    base_chain,
    get_session_history,
    input_messages_key="question",
    history_messages_key="history"
)

# 4Ô∏è‚É£ Invocar CON session_id
response = chain_with_history.invoke(
    {"question": "¬øCu√°l es mi nombre?"},
    config={"configurable": {"session_id": "user_123"}}
)

# El hist√≥rico se mantiene autom√°ticamente
```

### Con Mensajes del Sistema

```python
from langchain_core.messages import SystemMessage

system_prompt = SystemMessage(
    content="Eres un asistente de soporte t√©cnico. S√© conciso y √∫til."
)

# Mensaje del usuario
user_message = ("human", "{question}")

prompt = ChatPromptTemplate.from_messages([
    system_prompt,
    ("history", "{history}"),  # Placeholder para historial
    user_message
])

chain_with_history = RunnableWithMessageHistory(
    prompt | model | StrOutputParser(),
    get_session_history,
    input_messages_key="question",
    history_messages_key="history"
)
```

---

## Patr√≥n Agente (ReAct Loop)

Loop: Think ‚Üí Act ‚Üí Observe.

### Estructura

```python
from langchain.agents import create_react_agent, AgentExecutor
from langchain_core.tools import tool
from langchain_community.chat_models import ChatOllama

# 1Ô∏è‚É£ Definir herramientas
@tool
def search_documents(query: str) -> str:
    """Search knowledge base by query."""
    results = vectorstore.similarity_search(query, k=3)
    return "\n".join([doc.page_content for doc in results])

@tool
def calculate_sum(a: int, b: int) -> int:
    """Sum two numbers."""
    return a + b

tools = [search_documents, calculate_sum]

# 2Ô∏è‚É£ Crear agente
model = ChatOllama(model="llama3")
agent = create_react_agent(model, tools, prompt=agent_prompt)

# 3Ô∏è‚É£ Ejecutor
executor = AgentExecutor(
    agent=agent,
    tools=tools,
    max_iterations=5,
    verbose=True
)

# 4Ô∏è‚É£ Invocar
response = executor.invoke({
    "input": "¬øCu√°l es la capital de Francia? Suma 5 + 3."
})

print(response["output"])
```

### Ciclo Mental del Agente

```
Thought: Necesito buscar informaci√≥n sobre Francia.
Action: search_documents
Action Input: "Capital de Francia"

Observation: "Francia es un pa√≠s europeo. Par√≠s es la capital..."

Thought: Ya tengo la respuesta. Ahora calculo 5 + 3.
Action: calculate_sum
Action Input: a=5, b=3

Observation: 8

Thought: Puedo responder ahora.
Final Answer: La capital de Francia es Par√≠s. 5 + 3 = 8.
```

---

## Patr√≥n Paralelo (Multithreading)

Ejecutar m√∫ltiples runnables simult√°neamente.

### Estructura

```python
from langchain_core.runnables import RunnableParallel

# Crear m√∫ltiples an√°lisis en paralelo
parallel_analysis = RunnableParallel({
    "sentiment": sentiment_chain,
    "summary": summary_chain,
    "entities": entities_chain,
    "keywords": keywords_chain
})

# Invocar
text = "El producto es excelente pero el servicio fue lento..."
result = parallel_analysis.invoke({"text": text})

# Result:
# {
#     "sentiment": "Positivo con reservas",
#     "summary": "Buena calidad, mala velocidad",
#     "entities": ["producto", "servicio"],
#     "keywords": ["excelente", "lento"]
# }
```

### Caso Real: RAG con Re-ranking

```python
from langchain_core.runnables import RunnableParallel, RunnableLambda

# Buscar en paralelo: semantic + keyword search
retrieval = RunnableParallel({
    "semantic_docs": semantic_retriever,
    "keyword_docs": keyword_retriever
})

# Combinar y re-rankear
def combine_docs(parallel_result):
    semantic = parallel_result["semantic_docs"]
    keyword = parallel_result["keyword_docs"]
    # Custom logic: merge, deduplicate, rank
    return merge_and_rank(semantic, keyword)

full_chain = (
    retrieval
    | RunnableLambda(combine_docs)
    | prompt
    | model
    | parser
)
```

---

## Patr√≥n Condicional (Branching)

Ejecutar diferentes cadenas seg√∫n condici√≥n.

### Estructura

```python
from langchain_core.runnables import RunnableLambda, RunnableParallel

def route_based_length(input_dict):
    """Si texto es > 500 chars, summarize. Si no, pasar tal cual."""
    text = input_dict["text"]
    return "summarize" if len(text) > 500 else "passthrough"

# Cadenas alternativas
summarize_chain = prompt_summarize | model | parser
passthrough_chain = RunnableLambda(lambda x: x["text"])

# Selector con mapping
from langchain_core.runnables import RunnableBranch

branched_chain = RunnableBranch(
    (lambda x: len(x["text"]) > 500, summarize_chain),
    (lambda x: True, passthrough_chain)  # Default
)

# Uso
result = branched_chain.invoke({"text": "..." })
```

---

## Anti-Patterns y Errores Comunes

### ‚ùå ANTI-PATTERN 1: No Usar Pipe

```python
# ‚ùå BAD: Mezclar estilos
chain1 = LLMChain(llm=model, prompt=prompt)  # Legacy
chain2 = prompt | model | parser              # LCEL

# Inconsistencia: tipos diferentes, comportamientos diferentes
```

### ‚ùå ANTI-PATTERN 2: Ignorar Streaming

```python
# ‚ùå BAD: Esperar token por token innecesariamente
result = chain.invoke({"question": "..."})
print(result)  # Espera a TODOS los tokens antes de mostrar

# ‚úÖ GOOD: Streaming para UX fluida
for chunk in chain.stream({"question": "..."}):
    print(chunk, end="", flush=True)  # Muestra token por token
```

### ‚ùå ANTI-PATTERN 3: No Validar Outputs

```python
# ‚ùå BAD: Confiar en el LLM
response = model.invoke(prompt)
parsed = json.loads(response.content)  # ¬øQu√© si no es v√°lido JSON?

# ‚úÖ GOOD: Usar PydanticOutputParser
parser = PydanticOutputParser(pydantic_object=Schema)
chain = prompt | model | parser
parsed = chain.invoke({...})  # Validado autom√°ticamente
```

### ‚ùå ANTI-PATTERN 4: Bloquear en Async

```python
# ‚ùå BAD: Mezclar sync en async
async def process():
    result = chain.invoke({...})  # ‚Üê Blocking!

# ‚úÖ GOOD: Usar ainvoke
async def process():
    result = await chain.ainvoke({...})
```

---

## Checklist: Cadena Bien Formada

```bash
# ‚úÖ 1. Estructura
[ ] Usa pipe |
[ ] NO usa LLMChain/SimpleSequentialChain
[ ] Prompt es ChatPromptTemplate
[ ] Parser es espec√≠fico (StrOutputParser, PydanticOutputParser, etc)

# ‚úÖ 2. Inputs/Outputs
[ ] Inputs est√°n documentados
[ ] Outputs son type-safe
[ ] Errores de parsing tienen fallback

# ‚úÖ 3. Performance
[ ] Usa .stream() para UX fluida
[ ] .batch() para m√∫ltiples inputs
[ ] .astream() para async

# ‚úÖ 4. Testing
[ ] Mock model en tests
[ ] Edge cases cubiertos
[ ] Timeouts configurados

# ‚úÖ 5. Observabilidad
[ ] Prompts se loguean
[ ] Errores se capturan con contexto
[ ] LangSmith tracer integrado (opt)
```

---

**Fecha:** 30 de Enero de 2026
**Status:** ‚úÖ PRODUCTION-READY PATTERNS
**Responsable:** ArchitectZero AI Agent
