# üÜî Tech Profile: Ollama

> **Fecha:** 30/01/2026
> **Estado:** ‚úÖ Desplegado
> **Categor√≠a:** Local LLM Inference Engine
> **Web Oficial:** https://ollama.com/
> **Licencia:** MIT
> **Versi√≥n Recomendada:** 0.1.20+ (latest)

Motor para ejecutar Modelos de Lenguaje (LLMs) localmente sin internet. Es el **coraz√≥n de privacidad** en SoftArchitect AI: datos nunca salen de la m√°quina del usuario.

---

## üìñ Tabla de Contenidos

1. [Suitability (D√≥nde Brilla Ollama)](#suitability-d√≥nde-brilla-ollama)
2. [Value Analysis](#value-analysis)
3. [System Requirements](#system-requirements)
4. [Stack Integration](#stack-integration)
5. [Model Lifecycle & Versioning](#model-lifecycle--versioning)
6. [References & Ecosystem](#references--ecosystem)

---

## Suitability (D√≥nde Brilla Ollama)

### ‚úÖ Ideal Para

| Caso de Uso | Explicaci√≥n | Ejemplo SoftArchitect |
|:---|:---|:---|
| **Privacidad Total (GDPR/Medical)** | Datos nunca viajan a servidores de OpenAI/Anthropic. Cumplimiento regulatorio garantizado. | RAG sobre documentos t√©cnicos internos sin cargar a la nube |
| **Desarrollo R√°pido (Coste $0)** | Testear prompts, templates, chain-of-thought sin API credits. | Iterar en prompts de RAG sin gastar $$$$ en llamadas a GPT-4 |
| **RAG Local & Retrieval** | Embeddings + Inferencia en la misma m√°quina. Latencia <200ms garantizada. | Buscar en Knowledge Base + generar respuestas sin red |
| **Offline-First** | Funciona sin conexi√≥n a internet. Ideal para entornos corporativos aislados. | Empresa con pol√≠ticas anti-cloud puede usar SoftArchitect sin modificaciones |
| **Model Fine-Tuning** | Entrenar modelos en datos privados usando t√©cnicas como LoRA. | Especializar modelo en arquitectura de software SoftArchitect-specific |

### ‚ùå NO Usar Ollama Para

| Escenario | Por Qu√© | Alternativa |
|:---|:---|:---|
| **Producci√≥n Masiva Concurrente** | Ollama maneja una cola de requests. Escalabilidad limitada a ~10-20 queries/s en GPU modesta. | vLLM, TGI (Text Generation Inference), LiteLLM |
| **Modelos Gigantes sin GPU** | Llama-3-70b en CPU es **inservible** (100+ tokens/s). Timeout garantizado. | Cloud API (Azure OpenAI, Groq) o GPU dedicada |
| **Latencia <50ms cr√≠tica** | Ollama tiene overheads de desserializaci√≥n. RTT t√≠pico: 100-300ms. | Modelos optimizados (ONNX Runtime), edge inference (Triton) |
| **M√∫ltiples GPUs/Orchestraci√≥n** | Ollama no soporta distribuci√≥n multi-GPU nativa. | Kubernetes + vLLM, Ray Serve |

---

## Value Analysis

An√°lisis de Ollama en **6 dimensiones cr√≠ticas** para decisi√≥n t√©cnica.

### 1Ô∏è‚É£ Developer Speed (Velocidad de Setup)

**Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5 - Excelente)

```bash
# Install Ollama (1 comando)
curl https://ollama.ai/install.sh | sh

# Pull un modelo (1 comando)
ollama pull llama2:7b

# Listo para inferencia (segundos)
curl http://localhost:11434/api/generate -d '{"model":"llama2","prompt":"Hola"}'
```

**vs vLLM:** Requiere Git, CUDA setup, compilaci√≥n, virtualenv (30+ minutos).

---

### 2Ô∏è‚É£ Inference Performance (Velocidad de Generaci√≥n)

**Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê (4/5 - Bueno)

| M√©trica | Ollama | GPT-4 API | vLLM | Nota |
|:---|---:|---:|---:|:---|
| **Latencia First Token** | 500-800ms | 200-400ms | 300-500ms | Ollama sufre desserializaci√≥n inicial |
| **Throughput (tokens/s)** | 40-80 | 50-100 | 100-200 | GPU modesta (RTX 3070): ~60 tok/s |
| **Context Window** | 2k-8k | 8k-128k | 2k-32k | Configurable en Modelfile |
| **Cost (1M tokens)** | $0 (compute local) | $30 (input) | $0 (compute local) | Ollama gana en costo |

**Caso Real SoftArchitect:**
```
- RAG Query: Buscar 5 documentos relevantes (~1k tokens)
- Generaci√≥n: "Resume estos documentos" (~300 tokens)
- Latencia Total Ollama: 1.2s
- Latencia Total GPT-4 API: 2.5s + latencia red
```

---

### 3Ô∏è‚É£ Ease of Integration (Integraci√≥n con Stack)

**Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5 - Excelente)

Ollama expone **API REST compatible con OpenAI**. Cambiar de proveedor es reemplazar 1 variable de entorno:

```python
# client.py - Agn√≥stico de proveedor
from openai import OpenAI  # Misma API!

# üîß Integraci√≥n Ollama
client = OpenAI(
    api_key="not-needed",
    base_url="http://localhost:11434/v1"
)

# üîß Integraci√≥n OpenAI (swap autom√°tico)
# client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

response = client.chat.completions.create(
    model="llama2",  # Cambiar model_id solo
    messages=[{"role": "user", "content": "Hola"}]
)
```

**Ecosistema Compatible:**
- ‚úÖ LangChain (`llama2` model)
- ‚úÖ LiteLLM (proxy universal)
- ‚úÖ FastAPI clients (curl, requests)
- ‚úÖ Llamaindex (indexing)

---

### 4Ô∏è‚É£ Security & Privacy (Seguridad de Datos)

**Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5 - Excelente)

**Ventajas:**
- ‚úÖ Datos **nunca** salen del servidor local
- ‚úÖ No hay logging en servidores de terceros
- ‚úÖ Cumplimiento GDPR/HIPAA autom√°tico
- ‚úÖ Auditable: c√≥digo abierto (MIT)

**Riesgos:**
- ‚ö†Ô∏è Servidor Ollama expuesto a red corporativa (no firewalled)
- ‚ö†Ô∏è Modelo puede ser "exfiltrado" si puerto 11434 accesible

**Mitigaci√≥n (SoftArchitect):**
```dockerfile
# docker-compose.yml: Ollama SOLO accesible desde backend
services:
  ollama:
    expose:  # NO ports (no expose to host)
      - 11434
    networks:
      - softarchitect-net  # Red privada interna
```

---

### 5Ô∏è‚É£ Operational Maturity (Estabilidad)

**Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê (4/5 - Maduro)

**Fortalezas:**
- ‚úÖ Producci√≥n: 15,000+ descargas/d√≠a
- ‚úÖ Actualizado: releases mensuales
- ‚úÖ GPU: Soporte NVIDIA/AMD/Metal (Apple)

**Limitaciones:**
- ‚ö†Ô∏è No hay clustering multi-GPU nativo
- ‚ö†Ô∏è Escalabilidad vertical (m√°s RAM/VRAM) no horizontal
- ‚ö†Ô∏è Sin orquestador Kubernetes

---

### 6Ô∏è‚É£ Community & Ecosystem (Comunidad)

**Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê (4/5 - Muy Activa)

**Recursos:**
- ‚úÖ Discord activo (~50k miembros)
- ‚úÖ Model library: 100+ modelos precargados
- ‚úÖ GitHub: 50k+ stars, contribuciones regulares
- ‚úÖ Integraci√≥n con frameworks populares (LangChain, Llamaindex)

**Proyectos Relacionados:**
- **LiteLLM:** Proxy para unificar APIs (Ollama + OpenAI + Anthropic)
- **Llamaindex:** RAG framework con soporte Ollama
- **Ollama Web UI:** Frontend para gesti√≥n de modelos

---

## System Requirements

### M√≠nimos (Development)

| Componente | Requisito | Nota |
|:---|:---|:---|
| **CPU** | Intel i7/Ryzen 5+ | 4+ cores recomendado |
| **RAM** | 8GB m√≠nimo | 16GB recomendado |
| **GPU** | Opcional | CPU-only: ~5-10 tok/s |
| **Disk** | 20GB libre | Modelo Llama-2-7b: ~4GB |
| **OS** | Linux/macOS/Windows | Docker en Windows |

### Recomendado (Production SoftArchitect)

| Componente | Requisito | Justificaci√≥n |
|:---|:---|:---|
| **CPU** | 8+ cores | Embedding + generate concurrentemente |
| **RAM** | 32GB | FastAPI + Ollama + ChromaDB simult√°neo |
| **GPU** | NVIDIA RTX 4070+ (12GB VRAM) | Llama-2-13b inference |
| **Disk** | 100GB NVMe | Modelos + datos ChromaDB |
| **OS** | Linux (Ubuntu 22.04 LTS) | Stabilidad en producci√≥n |

### Hardware Benchmarks

| Hardware | Model | Throughput | Nota |
|:---|:---|---:|:---|
| **CPU (Intel i9)** | Llama-2-7b | 5 tok/s | Muy lento |
| **GPU (RTX 3070)** | Llama-2-7b | 60 tok/s | üëç Recomendado |
| **GPU (RTX 4070)** | Llama-2-13b | 80 tok/s | üëç Producci√≥n |
| **GPU (H100)** | Llama-2-70b | 200+ tok/s | üöÄ Enterprise |

---

## Stack Integration

### 1. Ollama ‚Üî FastAPI (Backend)

```python
# src/services/rag/llm_engine.py
from openai import OpenAI
from typing import AsyncGenerator

class OllamaLLMEngine:
    def __init__(self, base_url: str = "http://ollama:11434"):
        self.client = OpenAI(api_key="not-used", base_url=f"{base_url}/v1")
        self.model_id = "llama2:13b-chat"  # Custom Modelfile

    async def stream_generate(self, prompt: str) -> AsyncGenerator[str, None]:
        """Stream respuestas del LLM (evitar latencia de espera)"""
        response = self.client.chat.completions.create(
            model=self.model_id,
            messages=[
                {"role": "system", "content": "Eres un asistente de arquitectura de software."},
                {"role": "user", "content": prompt}
            ],
            stream=True,
            temperature=0.1,  # Determinista para RAG
        )
        for chunk in response:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
```

---

### 2. Ollama ‚Üî ChromaDB (Embeddings)

```python
# Opci√≥n A: Usar Ollama para embeddings (compatible)
from chromadb.config import Settings
from chromadb import Client

settings = Settings(
    chroma_db_impl="duckdb",
    anonymized_telemetry=False,
    allow_reset=True
)

client = Client(settings)
collection = client.get_or_create_collection(
    name="softarchitect_kb",
    metadata={"hnsw:space": "cosine"}
)

# ChromaDB intentar√° usar modelo de embedding local si est√° disponible
```

---

### 3. Ollama ‚Üî Docker (Orquestaci√≥n)

```yaml
# infrastructure/docker-compose.yml
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    environment:
      OLLAMA_MODELS: /models  # Persistencia
      OLLAMA_KEEP_ALIVE: 24h  # No descargar modelo tras inactividad
    volumes:
      - ollama_models:/root/.ollama/models
    ports:
      - "11434:11434"  # API
    networks:
      - softarchitect-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  backend:
    depends_on:
      - ollama  # Esperar a Ollama antes de iniciar API
    environment:
      OLLAMA_BASE_URL: "http://ollama:11434"

volumes:
  ollama_models:
    driver: local
```

---

## Model Lifecycle & Versioning

### Versiones de Ollama

| Versi√≥n | Fecha | Cambio Notable |
|:---|:---|:---|
| **0.0.1** | Ago 2023 | Lanzamiento inicial |
| **0.1.0** | Dic 2023 | Soporte metal (Apple Silicon) |
| **0.1.15** | May 2024 | Streaming improvements |
| **0.1.20+** | Ene 2025 | Estable para producci√≥n ‚úÖ |

**Recomendaci√≥n SoftArchitect:** Usar `0.1.20+`. Pin en docker-compose.

### Modelos Disponibles

```bash
# Listar modelos locales
ollama list

# Ejemplos de modelos peque√±os y r√°pidos:
ollama pull llama2:7b          # 3.8GB, r√°pido, general-purpose
ollama pull mistral:7b         # 4.0GB, mejor calidad, razonamiento
ollama pull neural-chat:7b     # 3.8GB, optimizado para chat
ollama pull openhermes:7b      # 3.8GB, mejor instruction-following

# Para documentaci√≥n t√©cnica:
ollama pull mistral:7b         # Mejor comprensi√≥n de textos t√©cnicos
```

---

## References & Ecosystem

### Documentaci√≥n Oficial
- https://github.com/ollama/ollama
- https://ollama.ai/library (Model library)

### Integraciones
- **LangChain:** https://python.langchain.com/docs/integrations/llms/ollama
- **Llamaindex:** https://gpt-index.readthedocs.io/en/latest/examples/llms/ollama.html
- **LiteLLM:** https://docs.litellm.ai/docs/providers/ollama

### Benchmarks & Comparativas
- https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard
- Model Card: https://ollama.ai/library/{{MODEL_NAME}}

---

## Conclusi√≥n

**Ollama es el motor de inferencia elegido para SoftArchitect porque:**

1. ‚úÖ **Privacidad:** Datos locales, sin nube
2. ‚úÖ **Velocidad:** Setup <5 minutos, inferencia <1s
3. ‚úÖ **Costo:** $0/mes en compute (amortizado en hardware)
4. ‚úÖ **Flexibilidad:** API compatible OpenAI, f√°cil swap

**Dogfooding Validation:** SoftArchitect usa Ollama para:
- Generar respuestas a queries del usuario (RAG)
- Fine-tune el modelo con patrones SoftArchitect internos (LoRA)
- Iterar prompts sin costos de API
