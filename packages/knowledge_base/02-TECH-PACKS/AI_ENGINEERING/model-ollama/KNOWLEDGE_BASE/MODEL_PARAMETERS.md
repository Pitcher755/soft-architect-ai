# üéõÔ∏è Ollama Model Parameters & Customization

> **Fecha:** 30/01/2026
> **Estado:** ‚úÖ Desplegado
> **Objetivo:** Afinar comportamiento del modelo LLM sin reentrenamiento
> **Audiencia:** Backend Engineers, ML/AI Specialists

C√≥mo configurar Ollama para que genere respuestas **deterministas, seguras y espec√≠ficas del dominio** SoftArchitect.

---

## üìã Tabla de Contenidos

1. [Par√°metros Cr√≠ticos](#par√°metros-cr√≠ticos)
2. [Modelfile: Crear Modelos Custom](#modelfile-crear-modelos-custom)
3. [Prompt Engineering Best Practices](#prompt-engineering-best-practices)
4. [Debugging & Validation](#debugging--validation)
5. [Pre-Production Checklist](#pre-production-checklist)

---

## Par√°metros Cr√≠ticos

### Control Central: Tabla de Par√°metros

| Par√°metro | Rango | Valor SoftArchitect | Efecto |
|:---|:---|:---|:---|
| **`temperature`** | 0.0 - 2.0 | **0.0 - 0.3** | Creatividad. 0=determinista (perfecta para RAG). 1.0=default. 2.0=aleatorio |
| **`num_ctx`** | 128 - 32k | **8192** | Ventana de contexto. Cu√°ntos tokens "recuerda". SoftArchitect inyecta 3-5 docs (~4k tokens) |
| **`num_predict`** | 1 - 32k | **500** | M√°ximo de tokens a generar. Evita responses infinitas |
| **`top_p`** | 0.0 - 1.0 | **0.9** | Nucleus sampling. 0.9=conservador. 1.0=sin filtro |
| **`top_k`** | 1 - 100 | **40** | Token diversity. Tomar top-40 tokens por probabilidad |
| **`repeat_penalty`** | 0.0 - 2.0 | **1.1** | Penalizar tokens repetidos. Evita loops |
| **`repeat_last_n`** | -1 - 256 | **64** | Cu√°ntos tokens mirar atr√°s para penalizar repetici√≥n |
| **`stop`** | string[] | `["User:", "Assistant:"]` | Tokens que detienen generaci√≥n. Crucial para multi-turn |

---

### Parameter Deep Dive

#### üå°Ô∏è Temperature (El M√°s Importante)

```
Temperature Spectrum:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 0.0 (Determinista) ‚Üí 2.0 (Ca√≥tico)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

0.0 - 0.3: RAG / C√≥digo / Decisiones cr√≠ticas
    ‚îî‚îÄ> Mismo prompt = MISMA respuesta siempre
    ‚îî‚îÄ> Perfecto para: RAG, generaci√≥n de code, respuestas basadas en hechos

0.5 - 0.7: Chat casual / Creativo controlado
    ‚îî‚îÄ> Variedad moderada manteniendo coherencia
    ‚îî‚îÄ> Perfecto para: Conversaci√≥n natural, brainstorming

0.8 - 1.5: Muy creativo / Storytelling
    ‚îî‚îÄ> Mucha variaci√≥n, a veces sin sentido
    ‚îî‚îÄ> Perfecto para: Ficci√≥n, poes√≠a, generaci√≥n de ideas

1.5 - 2.0: Caos puro (evitar en producci√≥n)
    ‚îî‚îÄ> Respuestas impredecibles, frecuentemente sin sentido
```

**Caso SoftArchitect:**
```python
# RAG (USAR temperature=0.0)
def generate_answer_for_rag(user_query: str, documents: List[str]):
    prompt = f"...{documents}...{user_query}"
    response = ollama_client.generate(
        model="softarchitect-rag",
        prompt=prompt,
        temperature=0.0  # ‚úÖ DETERMINISTA
    )

# Chat casual (USAR temperature=0.5)
def generate_chat_response(conversation_history: List[dict]):
    response = ollama_client.generate(
        model="softarchitect-chat",
        messages=conversation_history,
        temperature=0.5  # ‚úÖ VARIADO PERO COHERENTE
    )
```

---

#### üìñ `num_ctx` (Ventana de Contexto)

```
Contexto = "Cu√°nta historia recuerda el modelo"

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Input (Contexto) ‚Üí Model ‚Üí Output (Response) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

num_ctx=2048:  Cabe 1 documento peque√±o (~500 palabras)
num_ctx=4096:  Caben 2-3 documentos (~1500 palabras) ‚úÖ Default SoftArchitect
num_ctx=8192:  Caben 5+ documentos (~3000 palabras)
num_ctx=16384: Caben libro completo (~5000 palabras) ‚ö†Ô∏è Lento
```

**C√°lculo para SoftArchitect RAG:**

```
Doc 1: "Clean Architecture" (800 tokens)
Doc 2: "Pydantic V2 Validation" (600 tokens)
Doc 3: "Error Handling Patterns" (700 tokens)
Query: "C√≥mo estructurar un proyecto FastAPI?" (50 tokens)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total: ~2,150 tokens

‚úÖ Recomendaci√≥n: num_ctx=4096 (sobrante para prompt system)
```

---

#### üõë `stop` (Tokens de Parada - CR√çTICO para Multi-Turn)

```python
# ‚ùå BAD: Sin stop tokens
ollama_client.generate(
    model="llama2",
    prompt="User: ¬øQu√© es Clean Architecture?\nAssistant:"
)
# Resultado: El modelo genera "User: " de nuevo (loop!)

# ‚úÖ GOOD: Con stop tokens
ollama_client.generate(
    model="llama2",
    prompt="User: ¬øQu√© es Clean Architecture?\nAssistant:",
    stop=["User:", "\n\nUser:"]  # Detener si ve estos tokens
)
# Resultado: Genera solo la respuesta del asistente
```

---

### Presets por Caso de Uso

#### üéØ RAG (Respuestas Basadas en Documentos)

```python
rag_config = {
    "temperature": 0.0,      # Determinista
    "num_ctx": 4096,         # Contexto suficiente
    "num_predict": 300,      # Respuestas cortas
    "top_p": 0.9,            # Conservador
    "repeat_penalty": 1.2,   # Evitar repetici√≥n
    "stop": ["User:", "Query:"]
}

response = ollama_client.generate(
    model="llama2:13b-chat",
    prompt=system_prompt + context + user_query,
    **rag_config
)
```

#### üí¨ Chat Conversacional

```python
chat_config = {
    "temperature": 0.6,      # Ligeramente creativo
    "num_ctx": 2048,         # Historia reducida (m√°s r√°pido)
    "num_predict": 500,      # Respuestas moderadas
    "top_p": 0.95,           # M√°s natural
    "repeat_penalty": 1.1,   # Permitir algo de repetici√≥n
    "stop": ["User:", "\n\nUser:"]
}

response = ollama_client.generate(
    model="llama2:13b-chat",
    messages=conversation,
    **chat_config
)
```

#### üîç Code Generation

```python
code_config = {
    "temperature": 0.1,      # Muy determinista
    "num_ctx": 8192,         # Contexto amplio (necesita definiciones)
    "num_predict": 800,      # C√≥digo puede ser largo
    "top_p": 0.8,            # Menos ambig√ºedad en sintaxis
    "repeat_penalty": 1.3,   # Evitar duplicaci√≥n de c√≥digo
    "stop": ["def ", "class ", "\n\n# "]  # Detener entre funciones
}

response = ollama_client.generate(
    model="mistral:7b",
    prompt=system_prompt + code_context + request,
    **code_config
)
```

---

## Modelfile: Crear Modelos Custom

### Concepto: "Dockerfile para Modelos"

Un Modelfile es como un Dockerfile pero para modelos. Te permite:
1. Seleccionar modelo base
2. Configurar par√°metros por defecto
3. Inyectar System Prompt (personalidad)
4. Configurar tokens de parada

### ‚úÖ Ejemplo: Modelfile SoftArchitect

```dockerfile
# Modelfile
# Ubicaci√≥n: packages/knowledge_base/02-TECH-PACKS/AI_ENGINEERING/model-ollama/Modelfile

# 1. Seleccionar base
FROM mistral:7b

# 2. Par√°metros por defecto (RAG-optimized)
PARAMETER temperature 0.1
PARAMETER num_ctx 8192
PARAMETER num_predict 500
PARAMETER top_p 0.9
PARAMETER repeat_penalty 1.2
PARAMETER stop "User:"
PARAMETER stop "Query:"
PARAMETER stop "Assistant:"

# 3. System Prompt (Personalidad + Contexto)
SYSTEM """
Nombre: SoftArchitect AI
Rol: Asistente experto en Arquitectura de Software, patrones de dise√±o y desarrollo fullstack.

Instrucciones:
1. Responde EN ESPA√ëOL cuando sea posible, EN INGL√âS si el usuario pregunta en ingl√©s.
2. Sigue ESTRICTAMENTE Clean Architecture: Separation of Concerns, Dependency Inversion.
3. Prefiere soluciones comprobadas (SOLID, Domain-Driven Design) sobre hacks.
4. Si NO TIENES SUFICIENTE CONTEXTO, di: "No tengo informaci√≥n en mi base de conocimientos sobre esto."
5. SIEMPRE cita la fuente (ej: "Seg√∫n Clean Architecture de Uncle Bob...")
6. NUNCA inventes patrones o frameworks que no existan.
7. Para c√≥digo, sigue las reglas de SoftArchitect:
   - Type Hints obligatorios (Dart/Python)
   - @freezed/@immutable para modelos
   - Repository Pattern para acceso a datos
   - Testing: 70% unit, 20% integration, 10% e2e

Contexto de la Empresa:
- Stack: Python FastAPI (backend) + Flutter (frontend) + Docker (infra) + Ollama (IA)
- Dogfooding: SoftArchitect se construye a s√≠ mismo usando sus propios patrones
- Datos: PRIVADOS, nunca enviados a servidores externos
- Filosof√≠a: "Local-First, Privacy-First, Type-Safe-First"

Ejemplos de respuestas correctas:
‚úÖ "Para organizar tu c√≥digo FastAPI, usa Clean Architecture: core/, api/, domain/, infrastructure/"
‚úÖ "En Flutter, el estado debe manejarse con Riverpod AsyncNotifier, NO setState"
‚úÖ "Si no sabes, dilo. No alucines patrones."

Ejemplos de respuestas incorrectas:
‚ùå "Usa MVC porque es tradicional" (sin justificaci√≥n)
‚ùå "Aqu√≠ est√° tu soluci√≥n m√°gica:" (sin contexto)
‚ùå "Invent√© un patr√≥n llamado 'XyZPattern'" (no existe)
"""
```

### Crear e Instanciar Modelo Custom

```bash
# 1. Crear el modelo (compilar Modelfile)
ollama create softarchitect-v1 -f Modelfile

# 2. Verificar que existe
ollama list | grep softarchitect

# 3. Testear
ollama run softarchitect-v1

# Interactive prompt:
>>> ¬øQu√© es Clean Architecture?

# Salir
>>> /bye

# 4. Usar v√≠a API
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "softarchitect-v1",
    "prompt": "¬øCu√°les son los principios de Clean Architecture?",
    "stream": false
  }'
```

### Actualizar Modelo (Nueva Versi√≥n)

```bash
# 1. Modificar Modelfile (cambiar par√°metros, system prompt, etc.)
nano Modelfile

# 2. Versionar (v1 ‚Üí v2)
ollama create softarchitect-v2 -f Modelfile

# 3. Verificar ambas existen
ollama list | grep softarchitect

# 4. [Opcional] Eliminar versi√≥n anterior si no necesita
ollama rm softarchitect-v1

# 5. En SoftArchitect, cambiar variable de entorno
export LLM_MODEL=softarchitect-v2
```

---

## Prompt Engineering Best Practices

### Estructura de Prompt (3-Capas)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Layer 1: SISTEMA (System Prompt)           ‚îÇ ‚Üê En Modelfile
‚îÇ "Eres un experto en arquitectura..."       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Layer 2: CONTEXTO (Inyectado)              ‚îÇ ‚Üê Del RAG
‚îÇ "Seg√∫n Clean Architecture: ..."            ‚îÇ
‚îÇ "Documentos relevantes: [5 p√°rrafos]"      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Layer 3: QUERY (Usuario)                   ‚îÇ
‚îÇ "¬øC√≥mo estructuro un proyecto FastAPI?"   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### ‚úÖ GOOD: Prompt Bien Estructurado (RAG)

```python
def build_rag_prompt(
    user_query: str,
    retrieved_docs: List[str],
    system_context: str = None
) -> str:
    """Construcci√≥n segura de prompts para RAG"""

    system = system_context or """
Tu tarea es responder preguntas sobre arquitectura de software bas√°ndote SOLO en los documentos proporcionados.

Instrucciones:
1. Lee atentamente los DOCUMENTOS RELEVANTES
2. Extrae informaci√≥n factual (NO inventes)
3. Estructura la respuesta en secciones claras
4. Si NO HAY RESPUESTA en los documentos, di: "No encontr√© informaci√≥n al respecto en mis documentos"
5. SIEMPRE cita de d√≥nde sacaste la informaci√≥n
"""

    context = f"""
### DOCUMENTOS RELEVANTES:
{chr(10).join([f'---\n{doc}' for doc in retrieved_docs])}

### FIN DE DOCUMENTOS
"""

    query = f"""
### PREGUNTA DEL USUARIO:
{user_query}

Responde de forma concisa (m√°ximo 300 palabras), estructurada y con citas de los documentos.
"""

    return f"{system}\n\n{context}\n\n{query}"
```

### ‚ùå BAD: Prompt Desorganizado

```python
# ‚ùå NO HAGAS ESTO
prompt = f"""
{user_query}
{retrieved_docs}
Responde.
"""
# Problemas:
# - Sin separaci√≥n clara de secciones
# - Sin contexto al modelo de qu√© hacer
# - Sin l√≠mite de longitud
# - Sin instrucci√≥n de citar fuentes
```

---

### Few-Shot Prompting (Ejemplos)

```python
def build_few_shot_prompt(user_query: str) -> str:
    """Mostrar ejemplos de respuestas correctas al modelo"""

    examples = """
### EJEMPLOS DE RESPUESTAS CORRECTAS:

Pregunta: ¬øQu√© es el principio de Inversi√≥n de Dependencias?
Respuesta: El Principio de Inversi√≥n de Dependencias (DIP) establece que:
1. Las clases de alto nivel NO deben depender de clases de bajo nivel
2. Ambas deben depender de abstracciones
3. En SoftArchitect, usamos Repositorio Pattern e inyecci√≥n de dependencias (FastAPI Depends)
[Fuente: SOLID Principles - Uncle Bob]

Pregunta: ¬øC√≥mo estructuro un proyecto FastAPI?
Respuesta: Usa Clean Architecture con esta estructura:
src/
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ core/ (config, logger, exceptions)
‚îú‚îÄ‚îÄ api/ (routers HTTP)
‚îú‚îÄ‚îÄ domain/ (modelos, use cases, excepciones)
‚îú‚îÄ‚îÄ infrastructure/ (repos, DB, APIs externas)
[Fuente: SoftArchitect STRUCTURE_EXAMPLE.tree]
"""

    return f"""{examples}

### NUEVA PREGUNTA:
{user_query}

Responde siguiendo el mismo formato (conciso, estructurado, con fuente):
"""
```

---

## Debugging & Validation

### üîç C√≥mo Validar Que Ollama Est√° Funcionando

```bash
# 1. Verificar servicio corriendo
curl http://localhost:11434/api/tags
# Respuesta: {"models": [{"name": "llama2:7b", ...}]}

# 2. Test r√°pido (2-3 segundos)
ollama run llama2 "Di hola"

# 3. Monitorear logs
docker logs -f ollama  # Si corre en Docker

# 4. Check de recursos
nvidia-smi  # GPU usage (si tienes NVIDIA GPU)
free -h     # RAM usage

# 5. Benchmark de velocidad
time curl http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "Explica Clean Architecture en 100 palabras",
  "stream": false
}'
```

### ‚ö†Ô∏è Problemas Comunes

| Problema | S√≠ntoma | Soluci√≥n |
|:---|:---|:---|
| **Ollama no responde** | `curl: (7) Failed to connect` | Verificar: `docker ps`, puerto 11434 abierto |
| **Modelo no existe** | `"model 'llama2' not found"` | `ollama pull llama2:7b` |
| **VRAM insuficiente** | Respuestas muy lentas o crash | Reducir `num_ctx`, usar modelo 7b en lugar de 13b |
| **Respuestas sin sentido** | Output incoherente | Bajar `temperature`, aumentar `repeat_penalty` |
| **Loop infinito** | Modelo repite mismo texto | A√±adir `stop` tokens adecuados |

---

## Pre-Production Checklist

Antes de deployer Modelfile a producci√≥n:

```bash
# ‚úÖ 1. Validar Modelfile sintaxis
ollama create test-model -f Modelfile && echo "‚úÖ Sintaxis OK"

# ‚úÖ 2. Probar con queries RAG reales (3+ documentos)
curl -X POST http://localhost:11434/api/generate \
  -d '{"model":"test-model","prompt":"<your_rag_prompt>"}'

# ‚úÖ 3. Verificar determinismo (misma respuesta x2)
ollama run test-model "¬øQu√© es Clean Architecture?" > resp1.txt
ollama run test-model "¬øQu√© es Clean Architecture?" > resp2.txt
diff resp1.txt resp2.txt && echo "‚úÖ Determinista"

# ‚úÖ 4. Verificar stop tokens (no se corta mal)
# Generar prompt con multi-turn
ollama run test-model "User: Hola\nAssistant: Hola\nUser: ¬øQui√©n eres?"

# ‚úÖ 5. Performance benchmark (latencia <1s para 300 tokens)
time ollama run test-model "Explica en 300 palabras..."

# ‚úÖ 6. Test de cach√© (segunda llamada es m√°s r√°pida)
ollama run test-model "Hola"  # Fr√≠o
time ollama run test-model "¬øQu√© es Clean Architecture?"  # Caliente

# ‚úÖ 7. Liberar resources
ollama rm test-model

# ‚úÖ 8. Documentar par√°metros en comentario
# PAR√ÅMETROS FINALES (v1):
# - temperature: 0.0 (RAG-determinista)
# - num_ctx: 4096
# - temperature: 0.0
# - Tested: Ene 30, 2025 ‚úÖ
```

---

## Conclusi√≥n

**Los par√°metros de Ollama no son "m√°gicos"‚Äîson cient√≠ficos:**

1. ‚úÖ **Temperature=0.0** ‚Üí RAG = respuestas reproducibles
2. ‚úÖ **num_ctx=4096** ‚Üí Capacidad para 3-5 documentos t√©cnicos
3. ‚úÖ **stop tokens** ‚Üí Multi-turn coherente
4. ‚úÖ **System Prompt** ‚Üí Personalidad + contexto

**Dogfooding Validation:** Estos par√°metros se prueban cada vez que SoftArchitect genera una respuesta a un usuario.
