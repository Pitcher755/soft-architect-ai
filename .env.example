# .env.example
# SoftArchitect AI - Configuration Template
# Copy this to .env and modify as needed

# ========================
# PROJECT SETTINGS
# ========================
PROJECT_NAME=SoftArchitect_MVP
ENVIRONMENT=development

# ========================
# BACKEND (FastAPI)
# ========================
BACKEND_PORT=8000
BACKEND_HOST=0.0.0.0
DEBUG=True
LOG_LEVEL=INFO

# ========================
# AI ENGINE
# ========================
AI_PROVIDER=ollama
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=qwen2.5-coder:7b
OLLAMA_MEMORY_LIMIT=2GB

# ========================
# VECTOR DATABASE (ChromaDB)
# ========================
CHROMADB_HOST=chromadb
CHROMADB_PORT=8000
CHROMADB_PERSISTENCE=true
CHROMADB_DATA_PATH=/data/chromadb

# ========================
# RUNTIME SETTINGS
# ========================
IRON_MODE=true
OFFLINE_MODE=false

# ========================
# GPU CONFIGURATION
# ========================
# Configuración de GPU para Ollama (aceleración de LLM)
# Si tu equipo tiene GPU Nvidia:
#   1. Asegúrate de tener nvidia-docker2 instalado
#   2. En docker-compose.yml, verifica que la sección 'devices' de ollama esté descomentada
# Si NO tienes GPU:
#   1. En docker-compose.yml, comenta la sección 'devices' y descomenta 'devices: []'
#
# Esta variable es informativa (Docker Compose lee la config directamente del YAML)
GPU_ENABLED=true  # true para Nvidia RTX 3050, false para CPU-only
GPU_DEVICE_COUNT=1
