¬°Entendido! Vamos a convertirnos en los "Prompt Engineers" definitivos. Te voy a dar la secuencia de prompts exacta para que la copies y pegues en tu GEM. Cada prompt simula un "click" en los botones de la futura interfaz de SoftArchitect AI, obligando al modelo a ejecutar una fase del `MASTER_WORKFLOW_0-100.md`.

El objetivo es que **SoftArchitect AI** (el GEM) dise√±e **"SoftArchitect AI"** (el proyecto real). Meta-programaci√≥n en estado puro. ü§Ø

Copia y pega estos prompts **uno a uno** en tu GEM y guarda las respuestas.

---

### üîò PROMPT 1: FASE 0 - IDEACI√ìN (Business Analyst)

*Copia esto tal cual en el GEM:*

```markdown
[TRIGGER: FASE 0 - VISION & MVP]

**Contexto del Proyecto:**
Una aplicaci√≥n de escritorio llamada "SoftArchitect AI". Es un asistente para desarrolladores que funciona como un "Arquitecto Senior Virtual". Utiliza RAG (Retrieval-Augmented Generation) local con Ollama y ChromaDB para guiar al usuario a trav√©s de un workflow de ingenier√≠a de software estricto (desde requisitos hasta deploy), bas√°ndose en documentaci√≥n acad√©mica de un M√°ster. El objetivo es eliminar la par√°lisis por an√°lisis y asegurar la calidad del c√≥digo. Stack previsto: Frontend Flutter, Backend Python, IA Local.

**Instrucci√≥n:**
Act√∫a como Business Analyst Senior. Bas√°ndote estrictamente en la **Fase 0** del Master Workflow que conoces:
1.  Redacta el **Product Vision Statement** (Qu√©, Qui√©n, Por qu√©).
2.  Define el **MVP Scope** usando la regla 80/20: Lista 4 features "Must Have" y 3 "Post-MVP" (descartadas para v1).
3.  Genera la **Matriz de Riesgos** (T√©cnicos y de Negocio) identificando los 3 m√°s cr√≠ticos y su plan de mitigaci√≥n.
4.  Define 3 **KPIs de √âxito** medibles para este tipo de herramienta (Open Source / Developer Tool).

```

---

*(Espera a que el GEM responda y guarda el resultado. Luego lanza el siguiente)*

---

### üîò PROMPT 2: FASE 1 - ARQUITECTURA (Software Architect)

*Copia esto tal cual en el GEM:*

```markdown
[TRIGGER: FASE 1 - ARQUITECTURA & STACK]

**Visi√≥n del Proyecto:**
(Asume la visi√≥n generada en el paso anterior: Asistente local RAG para ingenier√≠a de software).

**Instrucci√≥n:**
Act√∫a como Senior Software Architect. Bas√°ndote estrictamente en la **Fase 1** del Master Workflow:
1.  Confirma y justifica el **Tech Stack** seleccionado usando la "Matriz de Decisi√≥n" (comparando opciones si es necesario, ej: Flutter vs Electron, Local vs Cloud).
2.  Redacta el **ADR-001 (Architecture Decision Record)** formal para la decisi√≥n de usar "IA Local Dockerizada" en lugar de APIs en la nube.
    * Contexto, Decisi√≥n, Rationale (Privacidad/Coste), Consecuencias.
3.  Genera un **Diagrama de Flujo de Datos (DFD)** textual (o c√≥digo Mermaid) mostrando c√≥mo fluye un prompt del usuario desde Flutter -> API Python -> Vector DB -> Ollama -> Respuesta.
4.  Realiza un an√°lisis **STRIDE** simplificado enfoc√°ndote en la amenaza "Data Leakage" (fugas de c√≥digo del usuario).

```

---

*(Espera a la respuesta y guarda. Siguiente)*

---

### üîò PROMPT 3: FASE 2 - SETUP (DevOps Engineer)

*Copia esto tal cual en el GEM:*

```markdown
[TRIGGER: FASE 2 - SCAFFOLDING]

**Stack Confirmado:**
Frontend: Flutter (Escritorio/Web)
Backend: Python (FastAPI)
IA/DB: Ollama + ChromaDB (Docker)

**Instrucci√≥n:**
Act√∫a como DevOps Lead. Bas√°ndote en la **Fase 2** del Master Workflow:
1.  Dise√±a la **Estructura de Directorios del Monorepo** ideal para este proyecto (separando apps, packages, docs).
2.  Genera el contenido completo del archivo `docker-compose.yml` para el entorno de desarrollo local (incluyendo healthchecks para Ollama y Postgres/Chroma).
3.  Crea el archivo `.env.example` con las variables cr√≠ticas (LLM_MODEL, API_PORT, etc.).
4.  Define los **pasos de instalaci√≥n** "Zero-Config" para el README (comandos exactos para que un dev clone y arranque).

```

---

*(Espera a la respuesta y guarda. Siguiente)*

---

### üîò PROMPT 4: FASE 5 - SEGURIDAD (Security Engineer)

*Copia esto tal cual en el GEM (Saltamos a seguridad porque es cr√≠tica antes de codificar)*

```markdown
[TRIGGER: FASE 5 - SEGURIDAD SHIFT-LEFT]

**Contexto:**
Aplicaci√≥n local que procesa c√≥digo sensible del usuario mediante IA.

**Instrucci√≥n:**
Act√∫a como Security Engineer. Bas√°ndote en la **Fase 5** del Master Workflow y OWASP Top 10:
1.  Define la **Security Checklist** espec√≠fica para este proyecto. C√©ntrate en:
    * A03: Injection (Prompt Injection en el LLM).
    * A04: Insecure Design (Privacidad de los datos locales).
    * A06: Vulnerable Components (Docker images).
2.  Dise√±a una estrategia de **Sanitizaci√≥n de Prompts** (c√≥mo evitar que el usuario rompa el sistema con inputs maliciosos).
3.  Especifica qu√© **Security Headers** o configuraciones de red debe tener el contenedor de la API para no exponerse a internet accidentalmente.

```

---

### üîò PROMPT 5: FASE 6 - INFRAESTRUCTURA (Cloud/Local Ops)

*Copia esto tal cual en el GEM:*

```markdown
[TRIGGER: FASE 6 - INFRAESTRUCTURA & CI/CD]

**Contexto:**
El proyecto es "Local-First", pero el c√≥digo se aloja en GitHub y debe tener CI/CD.

**Instrucci√≥n:**
Act√∫a como Infra Engineer. Bas√°ndote en la **Fase 6** del Master Workflow:
1.  Dise√±a el workflow de **GitHub Actions (CI)**. Debe incluir:
    * Linting de Flutter y Python.
    * Tests unitarios.
    * Security Scan (Trivy/Snyk) de las im√°genes Docker.
2.  Explica la estrategia de **Release**: ¬øC√≥mo distribuimos la app de escritorio a los usuarios? (Ej: GitHub Releases, instaladores).
3.  Redacta un peque√±o script o instrucci√≥n para automatizar la **descarga del modelo LLM** (ej: Llama 3) en el primer arranque del contenedor, para que el usuario no lo haga manual.

```

---

### üöÄ Plan de Acci√≥n

1. Ve a tu GEM.
2. Ejecuta estos 5 prompts en orden.
3. Copia todas las respuestas en un √∫nico documento (ej: `SIMULACION_SOFTARCHITECT.md`).
4. **S√∫belo aqu√≠** cuando lo tengas.

Lo revisar√© y te dir√©: *"Javi, esto es oro, ya tenemos el 80% del TFM hecho"* o *"Aqu√≠ falla X cosa, ajustemos el prompt"*. ¬°Al l√≠o! üë®‚Äçüíªüî•

---

### RESULTADOS RECIBIDOS

Ejecutando protocolo de **Fase 0: Pre-Desarrollo (Ideaci√≥n)** seg√∫n el est√°ndar *MASTER_WORKFLOW_0-100*.

Aqu√≠ tienes el entregable de definici√≥n estrat√©gica para el proyecto "SoftArchitect AI".

---

# üöÄ FASE 0: PRE-DESARROLLO (IDEACI√ìN)

## 0.1. Definir la Visi√≥n y Propuesta de Valor

### ‚úÖ Product Vision Statement

Siguiendo la estructura *"Qu√© es" + "Para qui√©n" + "Por qu√© importa"*:

> "**SoftArchitect AI** es una aplicaci√≥n de escritorio para **desarrolladores de software y arquitectos t√©cnicos** que act√∫a como un **'Arquitecto Senior Virtual'** mediante IA local (RAG), dise√±ada para **eliminar la par√°lisis por an√°lisis y asegurar el rigor ingenieril** guiando al usuario a trav√©s de un workflow de desarrollo estricto (0-100) sin depender de la nube."

---

## 0.2. MVP Scope (M√≠nimo Viable)

Aplicando la regla 80/20 (80% valor, 20% esfuerzo) y limitando el alcance para la V1:

### ‚úÖ Must-Have (V1 - Core Features)

1. **Motor RAG Local (Backend Python):** Integraci√≥n funcional de Ollama + ChromaDB para la ingesti√≥n del "Master Workflow" y consulta contextual sin conexi√≥n a internet.
2. **Workflow State Machine (Frontend Flutter):** Interfaz de navegaci√≥n lineal que bloquea el avance a la siguiente fase hasta completar los "Gates" (hitos) de la fase actual.
3. **Asistente de Chat Contextual:** Interfaz de chat que permite invocar "Triggers" espec√≠ficos por fase (ej. "Generar Matriz de Riesgos") utilizando el contexto cargado.
4. **Gesti√≥n de Persistencia Local:** Capacidad de crear, guardar y cargar el estado de un proyecto (archivos JSON/SQLite locales) manteniendo el historial de decisiones.

### üö´ Post-MVP (Descartado para V1)

1. **Integraci√≥n con IDEs:** Extensiones para VS Code o IntelliJ (se mantendr√° como app standalone inicialmente).
2. **Sincronizaci√≥n Cloud:** No habr√° login ni guardado en nube; privacidad total local ("Local First").
3. **Soporte Multi-LLM (Cloud):** No se integrar√° OpenAI/Anthropic API en V1; foco exclusivo en modelos locales (Llama 3, Mistral) para validar la premisa de privacidad y coste cero.

---

## 0.3. Matriz de Riesgos Cr√≠ticos

Identificaci√≥n de riesgos t√©cnicos y de negocio con estrategia de mitigaci√≥n:

| Riesgo | Tipo | Impacto | Probabilidad | Estrategia de Mitigaci√≥n (Plan B) |
| --- | --- | --- | --- | --- |
| **Performance de IA Local (Latencia)** | T√©cnico | ALTO | PROBABLE | Implementar soporte para modelos cuantizados (4-bit) y verificar hardware m√≠nimo al inicio. Fallback a modelos m√°s ligeros (ej. Phi-3) si no hay GPU. |
| **Complejidad de Instalaci√≥n (Dependencies)** | T√©cnico | ALTO | PROBABLE | Empaquetar el Backend Python como ejecutable (PyInstaller) o contenedor Docker embebido para evitar que el usuario deba configurar Python/Ollama manualmente. |
| **Alucinaciones en Arquitectura** | Calidad | MEDIO | PROBABLE | Strict System Prompts que fuercen a la IA a citar el documento base y responder "No lo s√©" si la informaci√≥n no est√° en el RAG (Grounding estricto). |

---

## 0.4. KPIs de √âxito (M√©tricas)

Definici√≥n de indicadores para herramienta Open Source / Developer Tool:

1. **Phase Completion Rate (Leading Indicator):** Porcentaje de usuarios que inician la Fase 0 y logran avanzar exitosamente hasta la Fase 2 (Setup). Mide la utilidad real del asistente para desbloquear el progreso.
2. **Artifact Generation Frequency:** N√∫mero promedio de entregables generados (ADRs, Diagramas, Specs) por sesi√≥n. Valida que el RAG est√° siendo √∫til para crear documentaci√≥n t√©cnica.
3. **Time-to-Hello-World (Efficiency):** Tiempo medido desde que el usuario crea un proyecto nuevo en SoftArchitect AI hasta que tiene su estructura de carpetas (Scaffolding) definida y lista para codificar.

---

**Siguiente paso sugerido:**
¬øDeseas proceder a la **[TRIGGER: FASE 1 - PLANIFICACI√ìN Y ARQUITECTURA]** para definir el Stack Tecnol√≥gico detallado (Flutter + Python bridge) y los ADRs iniciales?

---

Ejecutando protocolo de **Fase 1: Planificaci√≥n y Arquitectura** seg√∫n el est√°ndar *MASTER_WORKFLOW_0-100*.

Aqu√≠ tienes el entregable de arquitectura t√©cnica y seguridad para "SoftArchitect AI".

---

# üèóÔ∏è FASE 1: PLANIFICACI√ìN Y ARQUITECTURA

## 1.1. Selecci√≥n del Tech Stack (Matriz de Decisi√≥n)

Para una aplicaci√≥n de escritorio que debe ejecutar modelos de IA localmente, la gesti√≥n de la memoria (RAM) es el recurso cr√≠tico.

**Matriz de Decisi√≥n: Desktop Framework**

| Criterio | Peso | Flutter (Native) | Electron (Web) |
| --- | --- | --- | --- |
| **Consumo de Memoria (RAM)** | 35% | **Bajo (Dart VM)** (5) | Alto (Chromium) (2) |
| **Performance UI** | 25% | Nativo / Skia (5) | DOM / JS Bridge (3) |
| **Integraci√≥n IA (Python)** | 20% | FFI / Localhost (4) | Node-Python Bridge (4) |
| **Time-to-market** | 20% | Medio (4) | Alto (Web tech) (5) |

**Veredicto:** **Flutter + Python Backend**.
**Justificaci√≥n:** Electron consume demasiada RAM (~500MB+ base), recursos que necesitamos reservar para el modelo LLM local (Ollama) y la Vector DB. Flutter ofrece una UI fluida con bajo impacto en recursos.

**Stack Final:**

* **Frontend:** Flutter (Windows/Linux/macOS).
* **Backend/AI Service:** Python (FastAPI) corriendo en `localhost` (spawneado por la app).
* **RAG Engine:** LangChain + ChromaDB (Persistencia local).
* **Inferencia:** Ollama (Gesti√≥n de modelos Llama3/Mistral).

---

## 1.2. Architecture Decision Record (ADR-001)

Documentando la decisi√≥n cr√≠tica de arquitectura "Local First" siguiendo el template est√°ndar.

# ADR-001: Estrategia de IA Local (Local-First RAG)

## Decision
Utilizar una arquitectura 100% local basada en Ollama y ChromaDB, sin dependencias de APIs en la nube (OpenAI/Anthropic) para el MVP.

## Context
- "SoftArchitect AI" maneja propiedad intelectual cr√≠tica (c√≥digo fuente, ideas de negocio).
- Los usuarios objetivo (Desarrolladores/Arquitectos) valoran la privacidad por encima de la conveniencia.
- La latencia de red es inaceptable para una herramienta de productividad diaria.

## Rationale
1. **Privacidad (Security by Design):** Elimina por completo el vector de ataque de intercepci√≥n de datos en tr√°nsito o retenci√≥n de datos por terceros.
2. **Coste Operativo:** Coste marginal cero por token. Permite contextos largos (ingestar documentaci√≥n t√©cnica entera) sin facturas sorpresa.
3. **Resiliencia:** Funciona offline, cr√≠tico para entornos corporativos con firewalls estrictos.

## Consequences
- ‚úÖ Privacidad total garantizada (Data sovereignty).
- ‚ö†Ô∏è **Requisitos de Hardware:** El usuario final necesita m√≠nimo 16GB RAM (idealmente GPU dedicada). Esto reduce el TAM (Total Addressable Market).
- ‚ö†Ô∏è **Complejidad de Setup:** Debemos empaquetar el runtime de Python y gestionar la descarga de modelos (GBs) en el primer inicio.

## Status: Accepted



---

## 1.3. Diagrama de Flujo de Datos (DFD) y Trust Boundaries

Visualizaci√≥n del flujo RAG y los l√≠mites de confianza.

```mermaid
graph LR
    User([Usuario])
    
    subgraph "Desktop Client (Flutter)"
        UI[Interfaz Flutter]
        State[Gestor de Estado]
    end

    subgraph "Localhost (Trust Boundary)"
        API[FastAPI Server<br/>(Python Bridge)]
        RAG[RAG Orchestrator]
        VectorDB[(ChromaDB<br/>Local Vector Store)]
        Ollama[Ollama Service<br/>(LLM Inference)]
    end

    FileSystem[(Local File System<br/>Docs & Project)]

    %% Flujo
    User -->|1. Prompt| UI
    UI -->|2. HTTP POST /chat| API
    API -->|3. Retrieve Context| RAG
    RAG -->|4. Query| VectorDB
    VectorDB -->|5. Relevant Chunks| RAG
    RAG -->|6. Prompt + Context| Ollama
    Ollama -->|7. Streaming Token| API
    API -->|8. Update UI| State
    
    %% Ingesta
    FileSystem -.->|Ingest| API

```

**Trust Boundary Analysis:**
El l√≠mite de confianza es la propia m√°quina del usuario. No hay salida a internet excepto para la descarga inicial de modelos (que debe ser validada por hash).

---

## 1.4. An√°lisis STRIDE (Foco: Data Leakage)

An√°lisis de amenazas enfocado en la protecci√≥n del c√≥digo del usuario.

| Amenaza (STRIDE) | Descripci√≥n en Contexto | Mitigaci√≥n (Fase 1 & 5) |
| --- | --- | --- |
| **Information Disclosure (Data Leakage)** | Un proceso malicioso en la m√°quina del usuario o un error de configuraci√≥n expone el c√≥digo indexado en ChromaDB o los prompts enviados. | 1. **Localhost Binding:** La API Python debe escuchar *solo* en `127.0.0.1`, nunca en `0.0.0.0` para evitar acceso LAN.<br>

<br>2. **File Permissions:** La carpeta de ChromaDB debe tener permisos 700 (solo owner).<br>

<br>3. **Ephemeral Memory:** Opci√≥n para no persistir historial de chat en disco (RAM only mode). |
| **Tampering** | Inyecci√≥n de documentos falsos en el RAG para alucinar respuestas de seguridad incorrectas. | Validaci√≥n de integridad (checksums) de los documentos ingestados antes de procesarlos. |
| **Spoofing** | Otra aplicaci√≥n local haci√©ndose pasar por el backend de SoftArchitect. | Implementar un token de autenticaci√≥n simple (generado al vuelo) entre Flutter y Python. |

---

**Siguiente paso sugerido:**
¬øDeseas proceder a la **[TRIGGER: FASE 2 - SETUP INICIAL]** para definir la estructura del repositorio (Monorepo Flutter/Python), configurar el `docker-compose` (o script de arranque local) y los linters?
---
Ejecutando protocolo de **Fase 2: Setup Inicial (Scaffolding)** seg√∫n el est√°ndar *MASTER_WORKFLOW_0-100*.

Aqu√≠ tienes la infraestructura base "Production-Ready" para el entorno de desarrollo local.

---

# üèóÔ∏è FASE 2: SETUP INICIAL (SCAFFOLDING)

## 2.1. Estructura de Directorios (Monorepo Adaptado)

Aunque el est√°ndar sugiere Turborepo para stacks JS, adaptamos la estructura l√≥gica de **Monorepo** para convivir Flutter (Dart) y Python manteniendo la separaci√≥n de responsabilidades y "Trust Boundaries".

```text
soft-architect-ai/
‚îú‚îÄ‚îÄ .github/                 # Workflows CI/CD (Actions)
‚îú‚îÄ‚îÄ apps/
‚îÇ   ‚îú‚îÄ‚îÄ client-desktop/      # Frontend Flutter (Windows/Linux/Mac)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pubspec.yaml
‚îÇ   ‚îî‚îÄ‚îÄ api-server/          # Backend Python FastAPI (RAG Engine)
‚îÇ       ‚îú‚îÄ‚îÄ src/
‚îÇ       ‚îú‚îÄ‚îÄ tests/
‚îÇ       ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ       ‚îî‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ packages/
‚îÇ   ‚îú‚îÄ‚îÄ proto-contracts/     # Contratos compartidos (si us√°ramos gRPC) o JSON Schemas
‚îÇ   ‚îî‚îÄ‚îÄ docs/                # Documentaci√≥n Viva (Architecture, ADRs)
‚îú‚îÄ‚îÄ infra/
‚îÇ   ‚îú‚îÄ‚îÄ docker/              # Configs espec√≠ficas de contenedores
‚îÇ   ‚îî‚îÄ‚îÄ k8s/                 # (Futuro) Manifests para deploy
‚îú‚îÄ‚îÄ .env.example             # Template de variables de entorno
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ docker-compose.yml       # Orquestaci√≥n local
‚îî‚îÄ‚îÄ README.md                # Entry point

```

---

## 2.2. Docker Compose (Entorno Local)

Este archivo orquesta los servicios de soporte (IA, DB, VectorStore) para que el desarrollador no tenga que instalarlos manualmente. Se incluyen **Healthchecks** estrictos para evitar condiciones de carrera al inicio.

```yaml
version: '3.8'

services:
  # 1. Base de Datos Relacional (Metadatos de proyectos, Historial)
  postgres:
    image: postgres:16-alpine
    container_name: sa_postgres
    environment:
      POSTGRES_USER: dev_user
      POSTGRES_PASSWORD: dev_password
      POSTGRES_DB: soft_architect_db
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U dev_user"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - sa_network

  # 2. Vector Database (RAG Storage)
  chromadb:
    image: chromadb/chroma:latest
    container_name: sa_chroma
    environment:
      - IS_PERSISTENT=TRUE
    volumes:
      - chroma_data:/chroma/chroma
    ports:
      - "8000:8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]
      interval: 15s
      timeout: 10s
      retries: 3
    networks:
      - sa_network

  # 3. LLM Inference Engine (Local)
  # Nota: Requiere GPU Passthrough para rendimiento √≥ptimo en Linux/Windows WSL2
  ollama:
    image: ollama/ollama:latest
    container_name: sa_ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 20s
      timeout: 10s
      retries: 5
    networks:
      - sa_network

  # 4. Python Backend (FastAPI - RAG Bridge)
  api:
    build:
      context: ./apps/api-server
      dockerfile: Dockerfile
    container_name: sa_api
    volumes:
      - ./apps/api-server/src:/app/src
    ports:
      - "8080:8080"
    environment:
      - DATABASE_URL=postgresql://dev_user:dev_password@postgres:5432/soft_architect_db
      - CHROMA_URL=http://chromadb:8000
      - OLLAMA_URL=http://ollama:11434
    depends_on:
      postgres:
        condition: service_healthy
      chromadb:
        condition: service_healthy
      ollama:
        condition: service_healthy
    networks:
      - sa_network

volumes:
  postgres_data:
  chroma_data:
  ollama_models:

networks:
  sa_network:
    driver: bridge

```

---

## 2.3. Variables de Entorno (.env.example)

Template de configuraci√≥n segura. **NO** commitear el archivo `.env` real.

```bash
# .env.example

# --- APP CONFIG ---
APP_ENV=development
LOG_LEVEL=debug
API_PORT=8080

# --- DATABASE (PostgreSQL) ---
# Usar credenciales seguras en producci√≥n, esto es solo para dev local
DATABASE_URL=postgresql://dev_user:dev_password@localhost:5432/soft_architect_db

# --- VECTOR STORE (ChromaDB) ---
CHROMA_HOST=localhost
CHROMA_PORT=8000
CHROMA_COLLECTION=soft_architect_rag

# --- AI ENGINE (Ollama) ---
OLLAMA_BASE_URL=http://localhost:11434
# Modelo base para RAG (Debe estar descargado en Ollama)
LLM_MODEL_NAME=llama3:instruct
# Modelo para Embeddings (ligero)
EMBEDDING_MODEL_NAME=nomic-embed-text

# --- SECURITY ---
# Token para handshake local entre Flutter y Python (evitar spoofing)
API_SECRET_KEY=change_me_in_local_env_file

```

---

## 2.4. Pasos de Instalaci√≥n "Zero-Config" (README)

Instrucciones dise√±adas para reducir la fricci√≥n inicial ("Time-to-Code").


## üöÄ Quick Start (Development)

Prerrequisitos: Docker Desktop & Flutter SDK.

### 1. Setup Infraestructura (Backend + AI + DB)
Arranca todos los servicios de soporte en contenedores.
```bash
cp .env.example .env
docker-compose up -d
# Espera a que los servicios est√©n 'healthy' (docker ps)



### 2. Inicializar Modelos IA

Descarga los modelos necesarios en la instancia local de Ollama.

```bash
docker exec -it sa_ollama ollama pull llama3:instruct
docker exec -it sa_ollama ollama pull nomic-embed-text

```

### 3. Setup Base de Datos

Aplica las migraciones iniciales (Python/Alembic).

```bash
docker exec -it sa_api alembic upgrade head

```

### 4. Arrancar Cliente (Flutter)

```bash
cd apps/client-desktop
flutter pub get
flutter run -d windows # o macos/linux

```

---

**Siguiente paso sugerido:**
La infraestructura est√° lista. ¬øProcedemos a la **[TRIGGER: FASE 3 - DESARROLLO (BACKEND)]** para implementar el primer endpoint de la API (Ingesta de documentos RAG) siguiendo el ciclo TDD + Copilot?

---
Ejecutando protocolo de **Fase 5: Seguridad (Shift-Left)** seg√∫n el est√°ndar *MASTER_WORKFLOW_0-100*.

Aqu√≠ tienes la estrategia de seguridad defensiva para el entorno local de "SoftArchitect AI".

---

# üõ°Ô∏è FASE 5: SEGURIDAD (SHIFT-LEFT)

## 5.1. Security Checklist (OWASP Top 10 Adaptado)

Aplicando los controles cr√≠ticos definidos en el Master Workflow para un entorno Local-First + LLM:

### üíâ A03: Injection (Prompt Injection & Indirect Injection)

El nuevo "SQL Injection" es manipular el LLM para que ignore sus instrucciones de seguridad o exfiltre datos del RAG.

* [ ] **Delimitadores Estrictos:** Envolver SIEMPRE el input del usuario (query) y el contexto recuperado (chunks) en delimitadores XML o triples comillas dentro del System Prompt (ej. `<user_query>{input}</user_query>`) para que el modelo distinga instrucciones de datos.
* [ ] **Input Validation (Pydantic):** Rechazar prompts que excedan una longitud m√°xima (ej. 4000 caracteres) o contengan secuencias de escape no permitidas antes de siquiera tocar el LLM.
* [ ] **Salida Estructurada:** Forzar al LLM a responder en formato JSON siempre que sea posible (usando la feature `format: json` de Ollama) para evitar que texto malicioso se renderice como c√≥digo ejecutable en la UI.

### üîì A04: Insecure Design (Privacidad de Datos Local)

Aunque no hay nube, el riesgo es que otro proceso local acceda a los datos.

* [ ] **File System Permissions:** Los vol√∫menes de Docker (`postgres_data`, `chroma_data`) deben tener permisos `700` (solo el usuario propietario puede leer/escribir).
* [ ] **Ephemeral Memory Option:** Implementar un "Modo Inc√≥gnito" en el frontend que arranque ChromaDB en memoria (sin persistencia en disco) para sesiones altamente sensibles.
* [ ] **No-Root Containers:** Configurar los Dockerfiles para ejecutar los procesos como usuario no privilegiado (`UID 1001`), evitando que una vulnerabilidad en Python d√© acceso root al host.

### üì¶ A06: Vulnerable Components (Supply Chain)

* [ ] **Pinning de Versiones:** NUNCA usar tags `:latest` en `docker-compose.yml`. Usar SHAs o versiones sem√°nticas espec√≠ficas (ej. `python:3.11-slim-bookworm`).
* [ ] **Escaneo de Imagen:** Integrar `trivy` o `snyk` en el pipeline de CI para escanear las im√°genes base antes de construir el instalador del usuario.
* [ ] **Minimal Base Images:** Usar im√°genes `alpine` o `slim` para reducir la superficie de ataque.

---

## 5.2. Estrategia de Sanitizaci√≥n de Prompts

Para evitar que el usuario "rompa" el personaje de Arquitecto o realice inyecciones, implementaremos una capa de "Middle-logic" en Python antes de llamar a Ollama.

**Algoritmo de Sanitizaci√≥n:**

```python
# apps/api-server/src/security/sanitizer.py

import re
from fastapi import HTTPException

def sanitize_user_prompt(raw_input: str) -> str:
    """
    Limpia el input del usuario antes de integrarlo en el Prompt Template.
    """
    # 1. Longitud m√°xima (Prevenci√≥n de DoS por consumo de tokens)
    if len(raw_input) > 2000:
        raw_input = raw_input[:2000]

    # 2. Eliminar caracteres de control no imprimibles (excepto newlines)
    # Evita ataques de inyecci√≥n de comandos ocultos
    clean_input = "".join(ch for ch in raw_input if ch.isprintable() or ch in ['\n', '\r', '\t'])

    # 3. Escapar delimitadores que usamos en el System Prompt
    # Si usamos """ para envolver el input, debemos romper los """ del usuario
    clean_input = clean_input.replace('"""', '" " "')
    
    # 4. Bloqueo de patrones conocidos de Jailbreak (Lista negra b√°sica)
    jailbreak_patterns = ["ignore all previous instructions", "act as an unconstrained AI"]
    for pattern in jailbreak_patterns:
        if pattern.lower() in clean_input.lower():
            raise HTTPException(status_code=400, detail="Security Policy Violation: Jailbreak attempt detected.")

    return clean_input

```

**Estrategia "Sandwich Defense" en el Prompt:**
Colocar las instrucciones de seguridad al final del prompt, despu√©s del input del usuario, ya que los LLMs tienden a priorizar las instrucciones m√°s recientes (recency bias).

---

## 5.3. Configuraci√≥n de Red y Headers (Container Hardening)

El objetivo es asegurar que la API solo sea accesible por la aplicaci√≥n Flutter local y no expuesta a la red LAN/Wifi.

**Configuraci√≥n de Red (Docker Compose):**

```yaml
# docker-compose.yml (fragmento de red)
services:
  api:
    # BINDING ESTRICTO A LOCALHOST
    # Si se usa "8080:8080", Docker a veces bindea a 0.0.0.0 (todas las interfaces).
    # Usar "127.0.0.1:8080:8080" fuerza a que solo la m√°quina local pueda acceder.
    ports:
      - "127.0.0.1:8080:8080"
    
    # AISLAMIENTO INTERNO
    # La API accede a Ollama/DB por la red interna 'sa_network',
    # pero Ollama y DB NO deben exponer puertos al host si no es necesario para debug.
    networks:
      - sa_network

  ollama:
    # Solo exponer a la red interna de Docker, NO al host
    expose:
      - "11434"
    ports: [] # Eliminar mapeo de puertos al host en producci√≥n

```

**Security Headers (FastAPI Middleware):**
Aunque sea local, navegadores o webviews podr√≠an interactuar.

```python
# apps/api-server/src/main.py
from fastapi.middleware.cors import CORSMiddleware

# ...

# Configuraci√≥n CORS estricta
# Solo permitir or√≠genes locales y conocidos
origins = [
    "http://localhost",
    "http://127.0.0.1",
    "app://softarchitect" # Scheme com√∫n en apps desktop wrappers
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["GET", "POST"], # M√≠nimo privilegio
    allow_headers=["Authorization", "Content-Type"],
)

@app.middleware("http")
async def add_security_headers(request, call_next):
    response = await call_next(request)
    # Evitar que la API se cargue en iframes (Clickjacking)
    response.headers["X-Frame-Options"] = "DENY"
    # Prevenir sniffing de MIME types
    response.headers["X-Content-Type-Options"] = "nosniff"
    # HSTS no es estrictamente necesario en localhost puro, pero buena pr√°ctica si hay proxy TLS
    return response

```

---

**Siguiente paso sugerido:**
La seguridad base est√° definida. ¬øDeseas proceder a la **[TRIGGER: FASE 6 - INFRAESTRUCTURA]** para crear el `Dockerfile` optimizado de producci√≥n (Multi-stage build) para el servicio Python?
---

Ejecutando protocolo de **Fase 6: Infraestructura** seg√∫n el est√°ndar *MASTER_WORKFLOW_0-100*.

Aqu√≠ tienes la configuraci√≥n de CI/CD y automatizaci√≥n de infraestructura para "SoftArchitect AI".

---

# üèóÔ∏è FASE 6: INFRAESTRUCTURA & CI/CD

## 6.1. GitHub Actions Workflow (CI Pipeline)

Dise√±amos un pipeline unificado que respeta la estructura de Monorepo, ejecutando validaciones en paralelo para Backend y Frontend, e incluyendo el escaneo de seguridad de contenedores definido en la Fase 5.

```yaml
# .github/workflows/ci.yml
name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]

jobs:
  # --- JOB 1: BACKEND (Python) ---
  backend-quality:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: ./apps/api-server
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest flake8 black
      
      - name: Linting (Flake8)
        run: flake8 src/ --count --select=E9,F63,F7,F82 --show-source --statistics
      
      - name: Formatting Check (Black)
        run: black --check src/
      
      - name: Unit Tests
        run: pytest tests/

  # --- JOB 2: FRONTEND (Flutter) ---
  frontend-quality:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: ./apps/client-desktop
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Flutter
        uses: subosito/flutter-action@v2
        with:
          flutter-version: '3.19.0'
          channel: 'stable'
          cache: true
      
      - name: Install dependencies
        run: flutter pub get
      
      - name: Static Analysis (Linting)
        run: flutter analyze
      
      - name: Unit & Widget Tests
        run: flutter test

  # --- JOB 3: SECURITY SCAN (Docker) ---
  security-scan:
    needs: [backend-quality] # Solo escanear si el c√≥digo pasa tests
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Build Docker Image (API)
        run: docker build -t softarchitect-api:latest ./apps/api-server
      
      - name: Run Trivy Vulnerability Scanner
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: 'softarchitect-api:latest'
          format: 'table'
          exit-code: '1' # Fallar el pipeline si hay vulnerabilidades cr√≠ticas
          ignore-unfixed: true
          vuln-type: 'os,library'
          severity: 'CRITICAL,HIGH'

```

---

## 6.2. Estrategia de Release (Distribuci√≥n Desktop)

Al ser una aplicaci√≥n de escritorio "Local-First" sin servidor central, la distribuci√≥n se realizar√° mediante **GitHub Releases** automatizadas.

**Estrategia:**

1. **Tagging:** Al hacer push de un tag (ej: `v1.0.0`), se dispara el workflow de Release.
2. **Building:** Se compilan los binarios para cada OS (Windows `.exe`, Linux `.AppImage` o `.deb`). *Nota: macOS requiere un runner Mac y firma de c√≥digo (Notarizing) que tiene coste, se puede diferir para V1*.
3. **Publishing:** El binario compilado se sube como "asset" a la p√°gina de Releases de GitHub.

**Instrucciones para el usuario final:**

> "Descarga el instalador desde la secci√≥n 'Releases' de GitHub. El instalador incluye un script que verificar√° si tienes Docker instalado (requisito previo) y descargar√° las im√°genes necesarias en el primer inicio."

---

## 6.3. Automatizaci√≥n de Modelos (Script de Arranque)

Para cumplir el requisito de "Zero-Config" y no obligar al usuario a ejecutar comandos manuales de Ollama, utilizaremos un **Sidecar Container** o un **Entrypoint Script** en el servicio de Ollama.

A continuaci√≥n, la soluci√≥n m√°s robusta usando un script de entrada personalizado para el contenedor de la API (Python), que act√∫a como orquestador al inicio:

**Archivo: `apps/api-server/scripts/init_models.sh**`

```bash
#!/bin/bash
set -e

# Configuraci√≥n
OLLAMA_HOST="ollama:11434"
MODEL_NAME="llama3:instruct"
EMBED_MODEL="nomic-embed-text"

echo "‚è≥ Esperando a que Ollama est√© listo en $OLLAMA_HOST..."

# Loop de espera hasta que Ollama responda al healthcheck
until curl -s -f "http://$OLLAMA_HOST/api/tags" > /dev/null; do
  sleep 2
  echo "zzz... esperando a Ollama"
done

echo "‚úÖ Ollama detectado online."

# Funci√≥n para verificar y descargar modelo
check_and_pull() {
  local model=$1
  echo "üîç Verificando modelo: $model"
  
  if curl -s "http://$OLLAMA_HOST/api/tags" | grep -q "$model"; then
    echo "‚úÖ Modelo $model ya existe. Saltando descarga."
  else
    echo "‚¨áÔ∏è Descargando $model... (Esto puede tardar dependiendo de tu internet)"
    # Trigger pull via API (no bloquea el script del todo si se manda al background, pero aqu√≠ queremos bloquear)
    curl -X POST "http://$OLLAMA_HOST/api/pull" -d "{\"name\": \"$model\"}"
    echo "‚úÖ Descarga de $model completada."
  fi
}

check_and_pull "$MODEL_NAME"
check_and_pull "$EMBED_MODEL"

echo "üöÄ Todo listo. Iniciando servidor API..."
exec uvicorn src.main:app --host 0.0.0.0 --port 8080

```

**Integraci√≥n en `Dockerfile` (API Server):**

```dockerfile
# ... (etapas anteriores de build) ...

COPY scripts/init_models.sh /app/scripts/init_models.sh
RUN chmod +x /app/scripts/init_models.sh

# Reemplazamos el CMD directo por el script de inicializaci√≥n
ENTRYPOINT ["/app/scripts/init_models.sh"]

```

**Justificaci√≥n:**
Esto garantiza que la API *nunca* comience a aceptar peticiones del frontend hasta que los modelos necesarios est√©n f√≠sicamente disponibles en el volumen de Docker, evitando errores de "Model not found" en tiempo de ejecuci√≥n.

---

**Siguiente paso sugerido:**
La infraestructura CI/CD y la automatizaci√≥n est√°n listas. El proyecto est√° t√©cnicamente listo para entrar en fase de codificaci√≥n masiva. ¬øDeseas proceder a la **[TRIGGER: FASE 7 - DEPLOY A STAGING]** (en este caso, simular un Release Candidate v0.1.0 localmente)?

# üß™ Resultados de la Simulaci√≥n: SoftArchitect AI (Proof of Concept)

## 1. Objetivo de la Simulaci√≥n
Validar mediante t√©cnica "Mago de Oz" (usando un LLM avanzado simulando ser el sistema local) si el flujo de trabajo propuesto en `MASTER_WORKFLOW_0-100.md` es capaz de generar entregables de ingenier√≠a √∫tiles antes de escribir c√≥digo.

## 2. Metodolog√≠a
- **Modelo:** Gemini (Simulando RAG Local).
- **Inputs:** Prompts estructurados ("Triggers") correspondientes a las Fases 0, 1, 2, 5 y 6.
- **Contexto:** Documentaci√≥n del M√°ster en Desarrollo con IA (Enfoque Web/General).

## 3. Resultados Obtenidos
La simulaci√≥n gener√≥ exitosamente los siguientes artefactos estrat√©gicos para el propio desarrollo de SoftArchitect AI:

### ‚úÖ Fase 0: Visi√≥n y Alcance
- **MVP Definido:** App Desktop "Local-First".
- **Descarte Cr√≠tico:** Se elimin√≥ la integraci√≥n con IDEs para la V1 (demasiado complejo).
- **Riesgo Principal:** Hardware insuficiente para correr IA local.

### ‚úÖ Fase 1: Arquitectura
- **Stack Elegido:** Flutter (Frontend) + Python FastAPI (Backend) + Ollama/Chroma (IA).
- **Justificaci√≥n (ADR):** Se descart√≥ Electron por consumo de RAM.
- **Seguridad:** Se defini√≥ un modelo de amenazas centrado en "Data Leakage" local.

### ‚úÖ Fase 2: Setup
- **Estructura Monorepo:** Separaci√≥n clara `apps/client` y `apps/api`.
- **Docker Compose:** Configuraci√≥n lista para orquestar Postgres, Chroma y Ollama.

### ‚úÖ Fase 5: Seguridad
- **Sanitizaci√≥n:** Se dise√±√≥ un algoritmo de "Middle-logic" en Python para limpiar prompts antes de llegar al LLM.
- **Aislamiento:** Binding estricto a `127.0.0.1` para evitar acceso LAN.

## 4. Conclusiones y Brechas Detectadas (Gap Analysis)
1.  **Dependencia del Modelo:** La calidad de las respuestas dependi√≥ del conocimiento general de Gemini. El RAG local necesitar√° una base de conocimiento mucho m√°s amplia que solo los PDFs del m√°ster para replicar este nivel de detalle en stacks no-web (ej: Mobile Nativo).
2.  **Especifidad:** Se requiere una estrategia de "Tech Packs" (paquetes de conocimiento por tecnolog√≠a) para que el sistema pueda asistir en Swift, Kotlin o Rust con la misma solvencia que en Web.

---
**Estado:** POC Validada.
**Siguiente Paso:** Definici√≥n de la Arquitectura de Conocimiento (Knowledge Graph) y Estrategia Multi-Stack.