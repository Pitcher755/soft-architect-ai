# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ‹ Docker Compose Configuration - SoftArchitect AI
#
# OrquestaciÃ³n de servicios para desarrollo local (Linux/Windows/macOS)
#
# Arquitectura:
#   [Flutter App] <-> [FastAPI Backend] <-> {Ollama + ChromaDB}
#
# Requisitos:
#   - Docker Desktop o Docker Engine
#   - Docker Compose v2.0+
#   - 8GB RAM mÃ­nimo (4GB Ollama + 2GB ChromaDB + 2GB Sistema)
#   - GPU NVIDIA (opcional, para Ollama)
#
# Quick Start:
#   1. docker compose up --build
#   2. Esperar 30s para que Ollama descargue modelo
#   3. API disponible en http://localhost:8000
#   4. Swagger en http://localhost:8000/docs
#
# Refs:
#   - AGENTS.md: Arquitectura del proyecto
#   - context/30-ARCHITECTURE/TECH_STACK_DETAILS.es.md
#   - src/server/README.md: Backend documentation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

version: '3.9'

services:

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # ğŸ§  OLLAMA - IA Engine Local (Privacy-First)
  #
  # Servicio que ejecuta modelos de LLM localmente sin enviar
  # datos a internet. Privacidad total, latencia alta (~2-5s).
  #
  # Modelos soportados:
  #   - qwen2.5-coder:7b (recomendado para arquitectura de cÃ³digo)
  #   - llama2:7b (general purpose, mÃ¡s pequeÃ±o)
  #   - mistral:7b (excelente balance)
  #
  # GPU NVIDIA: Soporta CUDA con Container Toolkit instalado
  # CPU-only: FuncionarÃ¡ pero lento (~30s por respuesta)
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ollama:
    image: ollama/ollama:${OLLAMA_IMAGE_VERSION:-latest}
    container_name: sa_ollama
    
    # ğŸ“Œ PUERTO: NO exponer externamente, acceso vÃ­a red interna
    # Si necesitas acceso local: ports: ["11434:11434"]
    
    # ğŸ’¾ VOLÃšMENES: Persistencia de modelos descargados
    volumes:
      - ollama_storage:/root/.ollama
    
    # ğŸŒ RED: ComunicaciÃ³n interna con FastAPI
    networks:
      - sa_network
    
    # ğŸ”„ RESTART: Reintentar si falla (esperar 5s entre intentos)
    restart: on-failure:5
    
    # ğŸ›¡ï¸ RECURSOS: Limitar CPU/RAM para que no monopolice sistema
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: ${OLLAMA_MEMORY_LIMIT:-2GB}
        reservations:
          cpus: '1'
          memory: 1GB
        # GPU NVIDIA (OPCIONAL): Descomentar si tienes NVIDIA GPU
        # devices:
        #   - driver: nvidia
        #     count: 1
        #     capabilities: [gpu]
    
    # ğŸ“‹ HEALTHCHECK: Verificar que Ollama estÃ¡ listo
    healthcheck:
      test: curl -f http://localhost:11434/api/status || exit 1
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 20s  # Esperar a que descargue modelo
    
    # ğŸ“ LOGGING: Capturar logs en archivos del host
    logging:
      driver: 'json-file'
      options:
        max-size: '10m'
        max-file: '3'

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # ğŸ“š CHROMADB - Vector Database (Embeddings Locales)
  #
  # Almacenamiento vectorial para embeddings de documentaciÃ³n.
  # Permite bÃºsqueda semÃ¡ntica de Tech Packs.
  #
  # CaracterÃ­sticas:
  #   - Persistencia local en volumen Docker
  #   - Interfaz HTTP REST
  #   - No requiere credenciales
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  chromadb:
    image: chromadb/chroma:${CHROMADB_IMAGE_VERSION:-latest}
    container_name: sa_chromadb
    
    # ğŸ“Œ PUERTO: NO mapear a host (acceso solo vÃ­a red interna)
    # API accede a http://chromadb:8000
    # Si necesitas debug: ports: ["8001:8000"]
    
    # ğŸ’¾ VOLÃšMENES: Persistencia de embeddings
    volumes:
      - chroma_storage:/chroma/chroma
    
    # ğŸŒ RED: ComunicaciÃ³n interna con FastAPI
    networks:
      - sa_network
    
    # ğŸ”„ RESTART: Reintentar si falla
    restart: on-failure:3
    
    # ğŸ›¡ï¸ RECURSOS: Limitar para operaciÃ³n eficiente
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: ${CHROMADB_MEMORY_LIMIT:-512MB}
        reservations:
          cpus: '0.5'
          memory: 256MB
    
    # ğŸ“‹ HEALTHCHECK: Verificar disponibilidad
    healthcheck:
      test: curl -f http://localhost:8000/api/v1/heartbeat || exit 1
      interval: 10s
      timeout: 3s
      retries: 3
    
    # ğŸ“ LOGGING
    logging:
      driver: 'json-file'
      options:
        max-size: '10m'
        max-file: '2'

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # âš™ï¸ FASTAPI - Backend API (The Core)
  #
  # Servicio principal: orquestaciÃ³n RAG, routing de LLM,
  # gestiÃ³n de conversaciones, autenticaciÃ³n.
  #
  # Estructura:
  #   - app/main.py: Punto de entrada FastAPI
  #   - app/api/v1/: Endpoints versionados
  #   - app/core/: Config, seguridad, database
  #   - app/domain/: LÃ³gica de negocio (Pure Python)
  #
  # Puertos:
  #   - 8000: API REST + Swagger UI
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  api-server:
    # ğŸ”¨ BUILD: Compilar imagen desde Dockerfile
    build:
      context: ../src/server
      dockerfile: Dockerfile
      args:
        PYTHON_VERSION: ${PYTHON_VERSION:-3.12.3}
    
    container_name: sa_api
    
    # ğŸ“Œ PUERTOS: Mapear 8000 del contenedor al host
    ports:
      - "8000:8000"
    
    # ğŸ’¾ VOLÃšMENES: Hot-reload en desarrollo
    volumes:
      # CÃ³digo: Permite editar archivos y recargar sin rebuild
      - ../src/server:/app
      # ExclusiÃ³n: No sincronizar archivos de cachÃ© y venv
      - /app/.mypy_cache
      - /app/.pytest_cache
      - /app/venv
      # Knowledge Base: Acceso a Tech Packs (read-only)
      - ../packages/knowledge_base:/app/knowledge_base:ro
      # Logs: Persistir en host para debugging
      - ./logs:/app/logs
      # Data: Persistencia de SQLite y cachÃ©
      - ./data:/app/data
    
    # ğŸŒ RED: ComunicaciÃ³n con Ollama y ChromaDB
    networks:
      - sa_network
    
    # ğŸ”„ RESTART: Reintentar si crashea
    restart: on-failure:3
    
    # ğŸ›¡ï¸ RECURSOS: Limitar para estabilidad
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: ${API_MEMORY_LIMIT:-512MB}
        reservations:
          cpus: '0.5'
          memory: 256MB
    
    # ğŸ“‹ HEALTHCHECK: API listo (endpoint /api/v1/health)
    healthcheck:
      test: curl -f http://localhost:8000/api/v1/health || exit 1
      interval: 10s
      timeout: 3s
      retries: 5
      start_period: 10s
    
    # ğŸ”— DEPENDENCIAS: Esperar a que Ollama y ChromaDB estÃ©n listos
    depends_on:
      ollama:
        condition: service_healthy
      chromadb:
        condition: service_healthy
    
    # ğŸŒ VARIABLES DE AMBIENTE: ConfiguraciÃ³n en tiempo de ejecuciÃ³n
    environment:
      # Python: Logs no bufferizados (visibles en `docker logs`)
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      # FastAPI
      - DEBUG=${DEBUG:-False}
      - APP_NAME=SoftArchitect AI
      - APP_VERSION=0.1.0
      # LLM Configuration
      - LLM_PROVIDER=${LLM_PROVIDER:-local}
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=${OLLAMA_MODEL:-qwen2.5-coder:7b}
      - GROQ_API_KEY=${GROQ_API_KEY:-}
      - GROQ_MODEL=${GROQ_MODEL:-llama2-70b-4096}
      # Vector Store
      - CHROMA_HOST=chromadb
      - CHROMA_PORT=8000
      - CHROMADB_PATH=/chroma/chroma
      # Paths
      - KNOWLEDGE_BASE_PATH=/app/knowledge_base
      - CACHE_DIR=/app/data/cache
      - SQLITE_DB_PATH=/app/data/softarchitect.db
      # Security
      - IRON_MODE=${IRON_MODE:-True}
      - PII_DETECTION_ENABLED=${PII_DETECTION_ENABLED:-True}
    
    # ğŸ“ LOGGING
    logging:
      driver: 'json-file'
      options:
        max-size: '10m'
        max-file: '5'
        labels: 'service=api'

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ’¾ VOLÃšMENES PERSISTENTES (Nombrados)
# 
# Almacenamiento que persiste entre recreaciones de contenedores.
# UbicaciÃ³n: /var/lib/docker/volumes (Linux)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
volumes:
  
  # ğŸ§  Modelos descargados de Ollama (~5-10GB segÃºn modelo)
  ollama_storage:
    driver: local
  
  # ğŸ“š Embeddings de ChromaDB (~100MB tÃ­pico)
  chroma_storage:
    driver: local

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸŒ REDES PERSONALIZADAS
#
# Red interna para comunicaciÃ³n entre contenedores.
# Permite DNS automÃ¡tico: ollama:11434, chromadb:8000, etc.
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
networks:
  sa_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.25.0.0/16
